{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6edef2-9ff3-4e9c-a6a2-6a347ac5a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-08 18:58:42,223 [INFO] === Log initialized: /home/william/ai_digital_signage/audience-analysis-service/logs/audience_analysis_live.log ===\n",
      "2026-02-08 18:58:42,228 [INFO] Log file size: 272939 bytes\n",
      "2026-02-08 18:58:42,308 [INFO] BME688 sensor initialized (secondary)\n",
      "2026-02-08 18:58:42,317 [INFO] Loading models...\n",
      "2026-02-08 18:58:42,449 [INFO] Loaded WiderFace model\n",
      "2026-02-08 18:58:42,512 [INFO] Loaded embedding model\n",
      "2026-02-08 18:58:42,573 [INFO] Loaded age model\n",
      "2026-02-08 18:58:42,637 [INFO] Loaded gender model\n",
      "2026-02-08 18:58:42,698 [INFO] Loaded emotion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:36:53.989994342] [9512] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:330 \u001b[0mlibcamera v0.5.2+99-bfd68f78\n",
      "[0:36:53.999352460] [9595] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:720 \u001b[0mlibpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)\n",
      "[0:36:54.002662284] [9595] \u001b[1;32m INFO \u001b[1;37mIPAProxy \u001b[1;34mipa_proxy.cpp:180 \u001b[0mUsing tuning file /usr/share/libcamera/ipa/rpi/pisp/imx708.json\n",
      "[0:36:54.010844269] [9595] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:220 \u001b[0mAdding camera '/base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a' for pipeline handler rpi/pisp\n",
      "[0:36:54.010870807] [9595] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1179 \u001b[0mRegistered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a to CFE device /dev/media2 and ISP device /dev/media0 using PiSP variant BCM2712_D0\n",
      "[0:36:54.015191781] [9512] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1215 \u001b[0mconfiguring streams: (0) 640x480-RGB888/sRGB (1) 1536x864-BGGR_PISP_COMP1/RAW\n",
      "[0:36:54.015343226] [9595] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1483 \u001b[0mSensor: /base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a - Selected sensor format: 1536x864-SBGGR10_1X10/RAW - Selected CFE format: 1536x864-PC1B/RAW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-08 18:58:43,754 [INFO] Threaded camera started at 640x480 @ 10fps\n",
      "2026-02-08 18:58:43,759 [INFO] Demographics worker started (queue_size=8)\n",
      "2026-02-08 18:58:43,852 [INFO] Display available\n",
      "2026-02-08 18:58:43,857 [INFO] ============================================================\n",
      "2026-02-08 18:58:43,860 [INFO] GAZE TRACKING v3.3.0-PRODUCTION STARTED\n",
      "2026-02-08 18:58:43,863 [INFO]   Camera: 640x480 @ 10fps\n",
      "2026-02-08 18:58:43,867 [INFO]   Focal length: 482.5px (Pi Camera Module 3 calibrated)\n",
      "2026-02-08 18:58:43,870 [INFO]   Skip frames: 3 | Max faces: 3\n",
      "2026-02-08 18:58:43,873 [INFO]   Min loop time: 100ms\n",
      "2026-02-08 18:58:43,878 [INFO]   Thermal: throttle=78.0°C critical=82.0°C\n",
      "2026-02-08 18:58:43,881 [INFO]   Gaze (primary): Yaw ±30° Pitch ±20° Min 0.3s\n",
      "2026-02-08 18:58:43,887 [INFO]   Gaze (fallback): Yaw ±20.0° Pitch ±15.0° Center 0.6\n",
      "2026-02-08 18:58:43,891 [INFO]   Pose smoothing: alpha=0.4\n",
      "2026-02-08 18:58:43,895 [INFO]   Gaze Debug: DISABLED\n",
      "2026-02-08 18:58:43,901 [INFO]   Log file: /home/william/ai_digital_signage/audience-analysis-service/logs/audience_analysis_live.log\n",
      "2026-02-08 18:58:43,906 [INFO] ============================================================\n",
      "2026-02-08 18:58:49,316 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:49.316136Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 57.9, \"environment\": {\"temp_c\": 0.0, \"humidity\": 0.0, \"pressure_hPa\": 0.0, \"gas_resistance_ohms\": 0.0, \"noise_db\": 52.3}, \"gaze_diagnostics\": {\"frames_with_faces\": 0, \"kpts_valid\": 0, \"kpts_none\": 0, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 0, \"fallback_gazing\": 0, \"primary_gazing\": 0, \"total_gaze_checks\": 0, \"kpts_valid_pct\": 0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 0}}\n",
      "2026-02-08 18:58:49,321 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=59°C kpts_valid=0% pnp_ok=0% fallback=0%\n",
      "2026-02-08 18:58:54,966 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:54.965817Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 1, \"total_faces_detected\": 1, \"total_gaze_events\": 0, \"fps\": 9.7, \"cpu_temp\": 59.0, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 63.9}, \"gaze_diagnostics\": {\"frames_with_faces\": 1, \"kpts_valid\": 0, \"kpts_none\": 1, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 1, \"fallback_gazing\": 0, \"primary_gazing\": 0, \"total_gaze_checks\": 1, \"kpts_valid_pct\": 0.0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 100.0}}\n",
      "2026-02-08 18:58:55,009 [INFO] HEARTBEAT: gazers=0 tracked=1 faces_total=1 gaze_events=0 FPS=9.7 CPU=59°C kpts_valid=0.0% pnp_ok=0% fallback=100.0%\n",
      "2026-02-08 18:58:55,848 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:55.848552Z\", \"event\": \"gaze_start\", \"viewer_id\": \"2941ca1b\", \"demographics\": {\"age\": 0, \"gender\": \"\", \"emotion\": \"\"}, \"head_pose\": {\"yaw\": 14.19, \"pitch\": -8.23}}\n",
      "2026-02-08 18:58:55,852 [INFO] GAZE_START: 2941ca1b ( 0y)\n",
      "2026-02-08 18:58:58,101 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:58.101697Z\", \"event\": \"session_end\", \"viewer_id\": \"24e87239\", \"session_stats\": {\"viewer_id\": \"24e87239\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 4.97, \"total_appearances\": 1, \"engagement_rate\": 0.0, \"raw_engagement_rate\": 0.0}, \"demographics\": {\"age\": 32, \"gender\": \"Male\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 63.9}}\n",
      "2026-02-08 18:58:58,106 [INFO] SESSION END: 24e87239 gaze=0.0s dur=5.0s engage=0.0%\n",
      "2026-02-08 18:58:58,215 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:58.214976Z\", \"event\": \"session_end\", \"viewer_id\": \"2941ca1b\", \"session_stats\": {\"viewer_id\": \"2941ca1b\", \"total_gaze_time\": 3.1, \"gaze_count\": 1, \"session_duration\": 3.1, \"total_appearances\": 1, \"engagement_rate\": 1.0, \"raw_engagement_rate\": 1.0}, \"demographics\": {\"age\": 33, \"gender\": \"Female\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 63.9}}\n",
      "2026-02-08 18:58:58,219 [INFO] SESSION END: 2941ca1b gaze=3.1s dur=3.1s engage=100.0%\n",
      "2026-02-08 18:58:58,352 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:58.351953Z\", \"event\": \"gaze_start\", \"viewer_id\": \"e247f22b\", \"demographics\": {\"age\": 0, \"gender\": \"\", \"emotion\": \"\"}, \"head_pose\": {\"yaw\": 15.89, \"pitch\": -10.44}}\n",
      "2026-02-08 18:58:58,356 [INFO] GAZE_START: e247f22b ( 0y)\n",
      "2026-02-08 18:58:59,115 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:59.115590Z\", \"event\": \"session_end\", \"viewer_id\": \"1e412109\", \"session_stats\": {\"viewer_id\": \"1e412109\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 3.05, \"total_appearances\": 1, \"engagement_rate\": 0.0, \"raw_engagement_rate\": 0.0}, \"demographics\": {\"age\": 27, \"gender\": \"Male\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 63.9}}\n",
      "2026-02-08 18:58:59,119 [INFO] SESSION END: 1e412109 gaze=0.0s dur=3.0s engage=0.0%\n",
      "2026-02-08 18:58:59,977 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:58:59.976940Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 3, \"total_faces_detected\": 8, \"total_gaze_events\": 2, \"fps\": 8.8, \"cpu_temp\": 58.4, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 57.8}, \"gaze_diagnostics\": {\"frames_with_faces\": 8, \"kpts_valid\": 0, \"kpts_none\": 8, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 8, \"fallback_gazing\": 2, \"primary_gazing\": 0, \"total_gaze_checks\": 8, \"kpts_valid_pct\": 0.0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 100.0}}\n",
      "2026-02-08 18:58:59,983 [INFO] HEARTBEAT: gazers=0 tracked=3 faces_total=8 gaze_events=2 FPS=8.8 CPU=58°C kpts_valid=0.0% pnp_ok=0% fallback=100.0%\n",
      "2026-02-08 18:59:01,227 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:01.227005Z\", \"event\": \"gaze_start\", \"viewer_id\": \"c6abe1cf\", \"demographics\": {\"age\": 0, \"gender\": \"\", \"emotion\": \"\"}, \"head_pose\": {\"yaw\": 11.12, \"pitch\": -10.63}}\n",
      "2026-02-08 18:59:01,231 [INFO] GAZE_START: c6abe1cf ( 0y)\n",
      "2026-02-08 18:59:01,996 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:01.996088Z\", \"event\": \"session_end\", \"viewer_id\": \"e247f22b\", \"session_stats\": {\"viewer_id\": \"e247f22b\", \"total_gaze_time\": 0.3, \"gaze_count\": 1, \"session_duration\": 3.68, \"total_appearances\": 3, \"engagement_rate\": 0.0816, \"raw_engagement_rate\": 0.0816}, \"demographics\": {\"age\": 26, \"gender\": \"Male\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 57.8}}\n",
      "2026-02-08 18:59:02,000 [INFO] SESSION END: e247f22b gaze=0.3s dur=3.7s engage=8.2%\n",
      "2026-02-08 18:59:02,296 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:02.296251Z\", \"event\": \"session_end\", \"viewer_id\": \"452da806\", \"session_stats\": {\"viewer_id\": \"452da806\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 3.08, \"total_appearances\": 1, \"engagement_rate\": 0.0, \"raw_engagement_rate\": 0.0}, \"demographics\": {\"age\": 30, \"gender\": \"Male\", \"emotions\": {\"sadness\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 57.8}}\n",
      "2026-02-08 18:59:02,300 [INFO] SESSION END: 452da806 gaze=0.0s dur=3.1s engage=0.0%\n",
      "2026-02-08 18:59:02,596 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:02.596567Z\", \"event\": \"session_end\", \"viewer_id\": \"5831a710\", \"session_stats\": {\"viewer_id\": \"5831a710\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 3.08, \"total_appearances\": 1, \"engagement_rate\": 0.0, \"raw_engagement_rate\": 0.0}, \"demographics\": {\"age\": 30, \"gender\": \"Male\", \"emotions\": {\"sadness\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 57.8}}\n",
      "2026-02-08 18:59:02,600 [INFO] SESSION END: 5831a710 gaze=0.0s dur=3.1s engage=0.0%\n",
      "2026-02-08 18:59:03,033 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:03.033801Z\", \"event\": \"session_end\", \"viewer_id\": \"37c6477e\", \"session_stats\": {\"viewer_id\": \"37c6477e\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 3.04, \"total_appearances\": 1, \"engagement_rate\": 0.0, \"raw_engagement_rate\": 0.0}, \"demographics\": {\"age\": 33, \"gender\": \"Male\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.66, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 42780.75, \"noise_db\": 57.8}}\n",
      "2026-02-08 18:59:03,037 [INFO] SESSION END: 37c6477e gaze=0.0s dur=3.0s engage=0.0%\n",
      "2026-02-08 18:59:05,092 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:05.092093Z\", \"event\": \"heartbeat\", \"active_gazers\": 1, \"tracked_viewers\": 1, \"total_faces_detected\": 22, \"total_gaze_events\": 3, \"fps\": 10.0, \"cpu_temp\": 59.0, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.58, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 6880.24, \"noise_db\": 57.5}, \"gaze_diagnostics\": {\"frames_with_faces\": 22, \"kpts_valid\": 0, \"kpts_none\": 22, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 22, \"fallback_gazing\": 15, \"primary_gazing\": 0, \"total_gaze_checks\": 22, \"kpts_valid_pct\": 0.0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 100.0}, \"gazers\": [{\"viewer_id\": \"c6abe1cf\", \"continuous_gaze\": 3.6, \"total_gaze\": 0.0, \"demographics\": {\"age\": 32, \"gender\": \"Male\", \"emotion\": \"neutral\"}}]}\n",
      "2026-02-08 18:59:05,135 [INFO] HEARTBEAT: gazers=1 tracked=1 faces_total=22 gaze_events=3 FPS=10.0 CPU=57°C kpts_valid=0.0% pnp_ok=0% fallback=100.0%\n",
      "2026-02-08 18:59:10,149 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:10.149562Z\", \"event\": \"heartbeat\", \"active_gazers\": 1, \"tracked_viewers\": 1, \"total_faces_detected\": 38, \"total_gaze_events\": 3, \"fps\": 10.0, \"cpu_temp\": 57.9, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.58, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 6880.24, \"noise_db\": 56.9}, \"gaze_diagnostics\": {\"frames_with_faces\": 38, \"kpts_valid\": 0, \"kpts_none\": 38, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 38, \"fallback_gazing\": 31, \"primary_gazing\": 0, \"total_gaze_checks\": 38, \"kpts_valid_pct\": 0.0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 100.0}, \"gazers\": [{\"viewer_id\": \"c6abe1cf\", \"continuous_gaze\": 8.7, \"total_gaze\": 0.0, \"demographics\": {\"age\": 32, \"gender\": \"Male\", \"emotion\": \"neutral\"}}]}\n",
      "2026-02-08 18:59:10,193 [INFO] HEARTBEAT: gazers=1 tracked=1 faces_total=38 gaze_events=3 FPS=10.0 CPU=58°C kpts_valid=0.0% pnp_ok=0% fallback=100.0%\n",
      "2026-02-08 18:59:13,425 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:13.425879Z\", \"event\": \"session_end\", \"viewer_id\": \"c6abe1cf\", \"session_stats\": {\"viewer_id\": \"c6abe1cf\", \"total_gaze_time\": 12.23, \"gaze_count\": 1, \"session_duration\": 12.23, \"total_appearances\": 30, \"engagement_rate\": 1.0, \"raw_engagement_rate\": 1.0}, \"demographics\": {\"age\": 32, \"gender\": \"Male\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 31.69, \"humidity\": 53.58, \"pressure_hPa\": 1002.43, \"gas_resistance_ohms\": 6880.24, \"noise_db\": 56.9}}\n",
      "2026-02-08 18:59:13,468 [INFO] SESSION END: c6abe1cf gaze=12.2s dur=12.2s engage=100.0%\n",
      "2026-02-08 18:59:15,190 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-08T10:59:15.190087Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 39, \"total_gaze_events\": 3, \"fps\": 10.0, \"cpu_temp\": 59.5, \"environment\": {\"temp_c\": 31.68, \"humidity\": 53.6, \"pressure_hPa\": 1002.44, \"gas_resistance_ohms\": 12016.52, \"noise_db\": 55.0}, \"gaze_diagnostics\": {\"frames_with_faces\": 39, \"kpts_valid\": 0, \"kpts_none\": 39, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 39, \"fallback_gazing\": 32, \"primary_gazing\": 0, \"total_gaze_checks\": 39, \"kpts_valid_pct\": 0.0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 100.0}}\n",
      "2026-02-08 18:59:15,196 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=39 gaze_events=3 FPS=10.0 CPU=58°C kpts_valid=0.0% pnp_ok=0% fallback=100.0%\n",
      "2026-02-08 18:59:16,360 [INFO] Interrupted\n",
      "2026-02-08 18:59:16,365 [INFO] Shutting down...\n",
      "2026-02-08 18:59:16,373 [INFO] FINAL GAZE DIAGNOSTICS: {\"frames_with_faces\": 39, \"kpts_valid\": 0, \"kpts_none\": 39, \"solvepnp_success\": 0, \"solvepnp_fail\": 0, \"solvepnp_rejected\": 0, \"fallback_used\": 39, \"fallback_gazing\": 32, \"primary_gazing\": 0, \"total_gaze_checks\": 39, \"kpts_valid_pct\": 0.0, \"solvepnp_success_pct\": 0, \"fallback_pct\": 100.0}\n",
      "2026-02-08 18:59:16,401 [INFO] Clean shutdown. Total faces=39 gaze_events=3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Audience Attention and Gaze Analysis Module\n",
    "# ----------------------------------------------------------\n",
    "# Description:\n",
    "# This module implements real-time audience attention analysis for an\n",
    "# edge AI–powered digital signage system. Viewer attention is operationalised\n",
    "# through a composite Attention Indicator derived from gaze direction,\n",
    "# gaze duration, and face presence stability, enabling objective measurement\n",
    "# of on-screen engagement.\n",
    "#\n",
    "# In addition to visual analytics, ambient noise level is captured using\n",
    "# an external USB microphone as a contextual environmental indicator.\n",
    "# Noise measurements provide supplementary situational awareness of the\n",
    "# deployment environment (e.g., background activity and crowd conditions)\n",
    "# and are not used for speech recognition or audio content analysis.\n",
    "#\n",
    "# Functional Scope:\n",
    "# - Face detection and multi-face tracking using a WiderFace-based model\n",
    "#   with five-point facial landmark localisation\n",
    "# - Gaze estimation based on head pose and facial landmarks to infer\n",
    "#   on-screen viewing behaviour\n",
    "# - Attention Indicator computation combining gaze persistence and\n",
    "#   temporal face stability to mitigate transient or incidental detections\n",
    "# - Ambient noise level measurement for environmental context analysis\n",
    "# - Demographic estimation including age and gender classification\n",
    "# - Facial emotion recognition for affective context analysis\n",
    "# - Face embedding generation for short-term, privacy-preserving\n",
    "#   viewer session identification\n",
    "#\n",
    "# System Platform:\n",
    "# - Edge computing device: Raspberry Pi 5 (16 GB RAM)\n",
    "# - AI accelerator: Hailo-8 (26 TOPS)\n",
    "# - Image acquisition: Raspberry Pi Camera Module 3\n",
    "# - Audio input: USB microphone (ambient noise level measurement only)\n",
    "#\n",
    "# Research Context:\n",
    "# This module forms part of an AI-powered digital signage system evaluated\n",
    "# through controlled in-house experimentation and field-aligned testing\n",
    "# within Malaysian SME food and beverage (F&B) environments. The collected\n",
    "# visual and environmental indicators support quantitative analysis of\n",
    "# audience attention and engagement while adhering to privacy-by-design\n",
    "# principles.\n",
    "#\n",
    "# File: 000_audience_gaze_analysis.ipynb\n",
    "# Created: 07 February 2026\n",
    "# Version: 3.3.0\n",
    "# ----------------------------------------------------------\n",
    "#\n",
    "# CHANGELOG v3.3.0 (Gaze Estimation Fixes):\n",
    "# ---------------------------------------------------------\n",
    "# 1. FIXED: 3D reference points (_PTS3D) corrected to standard\n",
    "#    anthropometric proportions for 5-point facial landmarks.\n",
    "#    Previous values had incorrect Z-depth ratios causing\n",
    "#    systematic pose estimation bias.\n",
    "#\n",
    "# 2. FIXED: Euler angle extraction from rotation matrix now uses\n",
    "#    correct convention: yaw = Y-axis rotation (head left/right),\n",
    "#    pitch = X-axis rotation (head up/down), roll = Z-axis rotation\n",
    "#    (head tilt). Previous code computed Z-axis as \"yaw\", causing\n",
    "#    left/right head turns to not register correctly.\n",
    "#\n",
    "# 3. ADDED: Temporal smoothing (EMA) on yaw/pitch per viewer to\n",
    "#    reduce frame-to-frame noise from solvePnP jitter on 5-point\n",
    "#    landmarks. Configurable via POSE_SMOOTH_ALPHA.\n",
    "#\n",
    "# 4. FIXED: Camera intrinsic focal length calibrated for Pi Camera\n",
    "#    Module 3 (4.74mm focal length, 6.287mm sensor width) instead\n",
    "#    of arbitrary 0.9*width estimate.\n",
    "#\n",
    "# 5. ADDED: Diagnostic counters tracking keypoint availability rate,\n",
    "#    solvePnP success rate, and fallback usage rate. Logged every\n",
    "#    heartbeat for debugging gaze pipeline health.\n",
    "#\n",
    "# 6. CHANGED: MIN_GAZE_DURATION reduced from 0.5s to 0.3s based on\n",
    "#    literature (Ravnik & Solina, 2013: avg attention 0.7s).\n",
    "#\n",
    "# 7. ADDED: Fallback gaze now requires face to be within a tighter\n",
    "#    central region (FALLBACK_CENTER_RATIO) to reduce false positives.\n",
    "#\n",
    "# 8. ADDED: solvePnP solution validation — rejects poses with\n",
    "#    extreme angles (>80°) that indicate degenerate solutions.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "import threading\n",
    "import numpy as np\n",
    "import degirum as dg\n",
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "from datetime import datetime\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import bme680\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from hailo_platform import Device\n",
    "import sys\n",
    "from collections import deque\n",
    "import sounddevice as sd\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------------------------\n",
    "preview_camera = True\n",
    "console_output = True\n",
    "\n",
    "# Performance — tuned for RPi 5 + Hailo-8\n",
    "SKIP_FRAMES = 3\n",
    "CAMERA_FPS = 10\n",
    "PREVIEW_WIDTH = 640\n",
    "PREVIEW_HEIGHT = 480\n",
    "MAX_FACES_PER_FRAME = 3\n",
    "MIN_LOOP_TIME = 0.100\n",
    "DEMO_QUEUE_SIZE = 8\n",
    "\n",
    "# Thermal management\n",
    "THERMAL_THROTTLE_TEMP = 78.0\n",
    "THERMAL_CRITICAL_TEMP = 82.0\n",
    "THERMAL_CHECK_INTERVAL = 3.0\n",
    "\n",
    "# Gaze detection\n",
    "GAZE_YAW_THRESHOLD = 30  # ±30° horizontal\n",
    "GAZE_PITCH_THRESHOLD = 20  # ±20° vertical\n",
    "MIN_GAZE_DURATION = 0.3  # FIX #6: Reduced from 0.5s — literature reports avg 0.7s attention\n",
    "ENGAGEMENT_TIMEOUT = 3.0\n",
    "\n",
    "# Pose smoothing (FIX #3)\n",
    "POSE_SMOOTH_ALPHA = 0.4  # EMA factor: 0.0 = full smoothing, 1.0 = no smoothing\n",
    "\n",
    "# Fallback gaze tightening (FIX #7)\n",
    "FALLBACK_CENTER_RATIO = 0.6  # Face must be within central 60% of frame for fallback gaze=True\n",
    "FALLBACK_MAX_YAW = 20.0  # Tighter than primary method\n",
    "FALLBACK_MAX_PITCH = 15.0\n",
    "\n",
    "# Debug mode\n",
    "GAZE_DEBUG = False  # Set True for troubleshooting\n",
    "\n",
    "# Noise level (USB microphone)\n",
    "NOISE_SAMPLE_RATE = 16000\n",
    "NOISE_DURATION = 0.2\n",
    "NOISE_REF_PRESSURE = 20e-6\n",
    "NOISE_READ_INTERVAL = 5.0\n",
    "\n",
    "# Logging\n",
    "LOG_INTERVAL = 5.0\n",
    "OUTPUT_DIR = \"../output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# DeGirum / Hailo\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"../models\"\n",
    "token = \"\"\n",
    "device_type = \"HAILORT/HAILO8\"\n",
    "\n",
    "widerface_model_name = \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8_1\"\n",
    "face_embed_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8_1\"\n",
    "age_model_name = \"yolov8n_relu6_age--256x256_quant_hailort_hailo8_1\"\n",
    "gender_model_name = \"yolov8n_relu6_fairface_gender--256x256_quant_hailort_hailo8_1\"\n",
    "emotion_model_name = \"emotion_recognition_fer2013--64x64_quant_hailort_multidevice_1\"\n",
    "\n",
    "EMB_DIM = 128\n",
    "\n",
    "viewer_profiles = {}\n",
    "_ZERO_EMB = np.zeros(EMB_DIM, dtype=np.float32)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Gaze Diagnostic Counters (FIX #5)\n",
    "# ----------------------------------------------------------\n",
    "_gaze_diag = {\n",
    "    \"frames_with_faces\": 0,\n",
    "    \"kpts_valid\": 0,\n",
    "    \"kpts_none\": 0,\n",
    "    \"solvepnp_success\": 0,\n",
    "    \"solvepnp_fail\": 0,\n",
    "    \"solvepnp_rejected\": 0,  # extreme angles rejected\n",
    "    \"fallback_used\": 0,\n",
    "    \"fallback_gazing\": 0,\n",
    "    \"primary_gazing\": 0,\n",
    "    \"total_gaze_checks\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "def get_gaze_diagnostics():\n",
    "    \"\"\"Return a copy of gaze diagnostic counters.\"\"\"\n",
    "    d = _gaze_diag.copy()\n",
    "    total = d[\"kpts_valid\"] + d[\"kpts_none\"]\n",
    "    d[\"kpts_valid_pct\"] = round(100 * d[\"kpts_valid\"] / total, 1) if total > 0 else 0\n",
    "    pnp_total = d[\"solvepnp_success\"] + d[\"solvepnp_fail\"] + d[\"solvepnp_rejected\"]\n",
    "    d[\"solvepnp_success_pct\"] = round(100 * d[\"solvepnp_success\"] / pnp_total, 1) if pnp_total > 0 else 0\n",
    "    fb = d[\"fallback_used\"]\n",
    "    d[\"fallback_pct\"] = round(100 * fb / d[\"total_gaze_checks\"], 1) if d[\"total_gaze_checks\"] > 0 else 0\n",
    "    return d\n",
    "\n",
    "\n",
    "def reset_gaze_diagnostics():\n",
    "    \"\"\"Reset counters (call periodically if desired).\"\"\"\n",
    "    for k in _gaze_diag:\n",
    "        _gaze_diag[k] = 0\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Logging\n",
    "# ----------------------------------------------------------\n",
    "LOG_DIR = \"../logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "LOG_FILE = f\"{LOG_DIR}/audience_analysis_live.log\"\n",
    "\n",
    "\n",
    "class FlushingTimedRotatingFileHandler(TimedRotatingFileHandler):\n",
    "    \"\"\"TimedRotatingFileHandler that flushes + fsync after every emit.\"\"\"\n",
    "    def emit(self, record):\n",
    "        super().emit(record)\n",
    "        try:\n",
    "            self.flush()\n",
    "            if hasattr(self.stream, 'fileno'):\n",
    "                os.fsync(self.stream.fileno())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"audience_analysis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "logger.propagate = False\n",
    "\n",
    "_fmt = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "_fh = FlushingTimedRotatingFileHandler(\n",
    "    LOG_FILE, when=\"W0\", interval=1, backupCount=100, utc=True\n",
    ")\n",
    "_fh.setFormatter(_fmt)\n",
    "logger.addHandler(_fh)\n",
    "\n",
    "if console_output:\n",
    "    _ch = logging.StreamHandler(sys.stdout)\n",
    "    _ch.setFormatter(_fmt)\n",
    "    logger.addHandler(_ch)\n",
    "\n",
    "logger.info(f\"=== Log initialized: {os.path.abspath(LOG_FILE)} ===\")\n",
    "if os.path.exists(LOG_FILE):\n",
    "    logger.info(f\"Log file size: {os.path.getsize(LOG_FILE)} bytes\")\n",
    "\n",
    "\n",
    "def log_gaze_event(data):\n",
    "    \"\"\"Log structured JSON event.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"GAZE_EVENT: {json.dumps(data)}\")\n",
    "    except (TypeError, ValueError) as e:\n",
    "        logger.warning(f\"Serialize error: {e}\")\n",
    "\n",
    "\n",
    "def handle_uncaught_exceptions(exc_type, exc_value, exc_tb):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_tb)\n",
    "        return\n",
    "    logger.critical(\"Uncaught Exception\", exc_info=(exc_type, exc_value, exc_tb))\n",
    "\n",
    "sys.excepthook = handle_uncaught_exceptions\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CPU Temperature\n",
    "# ----------------------------------------------------------\n",
    "def get_cpu_temp():\n",
    "    try:\n",
    "        with open(\"/sys/class/thermal/thermal_zone0/temp\") as f:\n",
    "            return float(f.read().strip()) / 1000.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# BME688 Environmental Sensor\n",
    "# ----------------------------------------------------------\n",
    "bme_sensor = None\n",
    "bme_data_cache = {\n",
    "    \"temp_c\": 0.0, \"humidity\": 0.0,\n",
    "    \"pressure_hPa\": 0.0, \"gas_resistance_ohms\": 0.0\n",
    "}\n",
    "bme_last_read = 0.0\n",
    "\n",
    "\n",
    "def set_bme688_sensor(s):\n",
    "    s.set_humidity_oversample(bme680.OS_2X)\n",
    "    s.set_pressure_oversample(bme680.OS_4X)\n",
    "    s.set_temperature_oversample(bme680.OS_8X)\n",
    "    s.set_filter(bme680.FILTER_SIZE_3)\n",
    "    s.set_gas_status(bme680.ENABLE_GAS_MEAS)\n",
    "    s.set_gas_heater_temperature(320)\n",
    "    s.set_gas_heater_duration(150)\n",
    "    s.select_gas_heater_profile(0)\n",
    "\n",
    "\n",
    "BME_READ_INTERVAL = 10.0\n",
    "\n",
    "try:\n",
    "    bme_sensor = bme680.BME680(bme680.I2C_ADDR_PRIMARY)\n",
    "    set_bme688_sensor(bme_sensor)\n",
    "    logger.info(\"BME688 sensor initialized\")\n",
    "except (RuntimeError, IOError):\n",
    "    try:\n",
    "        bme_sensor = bme680.BME680(bme680.I2C_ADDR_SECONDARY)\n",
    "        set_bme688_sensor(bme_sensor)\n",
    "        logger.info(\"BME688 sensor initialized (secondary)\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"BME688 not available: {e}\")\n",
    "\n",
    "\n",
    "def read_bme688_data():\n",
    "    global bme_data_cache, bme_last_read\n",
    "    if bme_sensor is None:\n",
    "        return bme_data_cache\n",
    "    now = time.time()\n",
    "    if now - bme_last_read < BME_READ_INTERVAL:\n",
    "        return bme_data_cache\n",
    "    try:\n",
    "        if bme_sensor.get_sensor_data():\n",
    "            bme_data_cache = {\n",
    "                \"temp_c\": round(bme_sensor.data.temperature, 2),\n",
    "                \"humidity\": round(bme_sensor.data.humidity, 2),\n",
    "                \"pressure_hPa\": round(bme_sensor.data.pressure, 2),\n",
    "                \"gas_resistance_ohms\": round(bme_sensor.data.gas_resistance, 2)\n",
    "            }\n",
    "            bme_last_read = now\n",
    "    except Exception:\n",
    "        pass\n",
    "    return bme_data_cache\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Ambient Noise Level\n",
    "# ----------------------------------------------------------\n",
    "_noise_cache = {\"db\": None, \"ts\": 0.0}\n",
    "\n",
    "def read_noise_level_db():\n",
    "    now = time.time()\n",
    "    if now - _noise_cache[\"ts\"] < NOISE_READ_INTERVAL:\n",
    "        return _noise_cache[\"db\"]\n",
    "\n",
    "    try:\n",
    "        audio = sd.rec(\n",
    "            int(NOISE_SAMPLE_RATE * NOISE_DURATION),\n",
    "            samplerate=NOISE_SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32',\n",
    "            blocking=True\n",
    "        )\n",
    "\n",
    "        rms = np.sqrt(np.mean(np.square(audio)))\n",
    "        if rms > 0:\n",
    "            db = 20 * np.log10(rms / NOISE_REF_PRESSURE)\n",
    "            db = round(float(db), 1)\n",
    "        else:\n",
    "            db = 0.0\n",
    "\n",
    "        _noise_cache.update({\"db\": db, \"ts\": now})\n",
    "        return db\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Noise read failed: {e}\")\n",
    "        return _noise_cache[\"db\"]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Threaded Camera Capture\n",
    "# ----------------------------------------------------------\n",
    "class ThreadedCamera:\n",
    "    def __init__(self, fps=10, width=640, height=480):\n",
    "        self.picam2 = Picamera2()\n",
    "        config = self.picam2.create_preview_configuration(\n",
    "            main={\"format\": \"RGB888\", \"size\": (width, height)},\n",
    "            controls={\"FrameRate\": fps}\n",
    "        )\n",
    "        self.picam2.configure(config)\n",
    "        self.picam2.start(show_preview=False)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        self._frame = None\n",
    "        self._lock = threading.Lock()\n",
    "        self._running = True\n",
    "        self._interval = 1.0 / fps\n",
    "\n",
    "        self._thread = threading.Thread(target=self._capture_loop, daemon=True, name=\"cam-capture\")\n",
    "        self._thread.start()\n",
    "        logger.info(f\"Threaded camera started at {width}x{height} @ {fps}fps\")\n",
    "\n",
    "    def _capture_loop(self):\n",
    "        while self._running:\n",
    "            try:\n",
    "                start = time.time()\n",
    "                frame = self.picam2.capture_array()\n",
    "                with self._lock:\n",
    "                    self._frame = frame\n",
    "                elapsed = time.time() - start\n",
    "                sleep = max(0, self._interval - elapsed)\n",
    "                if sleep > 0:\n",
    "                    time.sleep(sleep)\n",
    "            except Exception:\n",
    "                if self._running:\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "    def read(self):\n",
    "        with self._lock:\n",
    "            return self._frame\n",
    "\n",
    "    def stop(self):\n",
    "        self._running = False\n",
    "        self._thread.join(timeout=2.0)\n",
    "        try:\n",
    "            self.picam2.stop()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Utilities\n",
    "# ----------------------------------------------------------\n",
    "def cosine_distance(a, b):\n",
    "    d = np.dot(a, b)\n",
    "    n = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return 1.0 - (d / n) if n > 0 else 1.0\n",
    "\n",
    "\n",
    "def parse_keypoints(kpts_data):\n",
    "    if kpts_data is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(kpts_data, np.ndarray):\n",
    "            if kpts_data.shape == (5, 2):\n",
    "                return kpts_data.astype(np.float32)\n",
    "            if kpts_data.ndim == 1 and kpts_data.size == 10:\n",
    "                return kpts_data.reshape(5, 2).astype(np.float32)\n",
    "        elif isinstance(kpts_data, (list, tuple)):\n",
    "            a = np.array(kpts_data, dtype=np.float32)\n",
    "            if a.size == 10:\n",
    "                return a.reshape(5, 2)\n",
    "            if a.shape == (5, 2):\n",
    "                return a\n",
    "        elif isinstance(kpts_data, dict):\n",
    "            if 'data' in kpts_data:\n",
    "                return parse_keypoints(kpts_data['data'])\n",
    "            if 'x' in kpts_data and 'y' in kpts_data:\n",
    "                x = np.array(kpts_data['x'], dtype=np.float32)\n",
    "                y = np.array(kpts_data['y'], dtype=np.float32)\n",
    "                if len(x) == 5 and len(y) == 5:\n",
    "                    return np.column_stack([x, y])\n",
    "        elif isinstance(kpts_data, list) and kpts_data and isinstance(kpts_data[0], dict):\n",
    "            c = []\n",
    "            for pt in kpts_data[:5]:\n",
    "                if 'x' in pt and 'y' in pt:\n",
    "                    c.append([float(pt['x']), float(pt['y'])])\n",
    "            if len(c) == 5:\n",
    "                return np.array(c, dtype=np.float32)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_embedding(emb_data):\n",
    "    try:\n",
    "        if isinstance(emb_data, np.ndarray):\n",
    "            return emb_data.flatten().astype(np.float32)\n",
    "        if isinstance(emb_data, dict):\n",
    "            for k in (\"data\", \"embedding\", \"vector\"):\n",
    "                v = emb_data.get(k)\n",
    "                if v is not None:\n",
    "                    if isinstance(v, np.ndarray):\n",
    "                        return v.flatten().astype(np.float32)\n",
    "                    if isinstance(v, list) and v:\n",
    "                        if isinstance(v[0], list):\n",
    "                            v = v[0]\n",
    "                        return np.array(v, dtype=np.float32)\n",
    "        if isinstance(emb_data, list) and emb_data:\n",
    "            if isinstance(emb_data[0], list):\n",
    "                emb_data = emb_data[0]\n",
    "            return np.array(emb_data, dtype=np.float32)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return _ZERO_EMB.copy()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Head Pose Estimation — CORRECTED (FIX #1, #2, #4, #8)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# FIX #1: Corrected 3D reference points for 5-point facial landmarks\n",
    "# Standard anthropometric model (mm): left_eye, right_eye, nose_tip,\n",
    "# left_mouth_corner, right_mouth_corner.\n",
    "# Coordinate system: X=right, Y=down, Z=forward (out of face).\n",
    "# Previously the nose tip had Z=60 (too far forward) and mouth corners\n",
    "# had Z=20 (behind eyes at Z=30), creating incorrect depth ratios.\n",
    "_PTS3D = np.array([\n",
    "    [-30.0,  -30.0,  -30.0],   # Left eye corner\n",
    "    [ 30.0,  -30.0,  -30.0],   # Right eye corner\n",
    "    [  0.0,    0.0,    0.0],   # Nose tip (origin / most forward point)\n",
    "    [-25.0,   30.0,  -20.0],   # Left mouth corner\n",
    "    [ 25.0,   30.0,  -20.0],   # Right mouth corner\n",
    "], dtype=np.float64)\n",
    "\n",
    "_DIST = np.zeros(4, dtype=np.float64)\n",
    "\n",
    "# FIX #4: Calibrated focal length for Pi Camera Module 3\n",
    "# Sensor: Sony IMX708, focal length = 4.74mm, sensor width = 6.287mm\n",
    "# At 640px width: fx = 640 * 4.74 / 6.287 ≈ 482.8\n",
    "# At 480px height: fy = 480 * 4.74 / 4.712 ≈ 482.8 (square pixels)\n",
    "# Previous value: 0.9 * 640 = 576 (overestimated by ~19%)\n",
    "_FOCAL_LENGTH = 640.0 * 4.74 / 6.287  # ≈ 482.8 pixels\n",
    "_K = np.array([\n",
    "    [_FOCAL_LENGTH,           0, PREVIEW_WIDTH / 2.0],\n",
    "    [0,           _FOCAL_LENGTH, PREVIEW_HEIGHT / 2.0],\n",
    "    [0,                       0, 1.0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "# Per-viewer pose smoothing state (FIX #3)\n",
    "_pose_history = {}  # viewer_id -> {\"yaw\": float, \"pitch\": float, \"roll\": float}\n",
    "\n",
    "\n",
    "def _smooth_pose(vid, yaw, pitch, roll):\n",
    "    \"\"\"\n",
    "    Apply exponential moving average (EMA) to head pose angles per viewer.\n",
    "    Reduces frame-to-frame jitter from noisy 5-point solvePnP estimates.\n",
    "    \n",
    "    FIX #3: Without smoothing, single-frame landmark jitter can flip\n",
    "    gaze state on/off rapidly, producing unreliable attention metrics.\n",
    "    \"\"\"\n",
    "    alpha = POSE_SMOOTH_ALPHA\n",
    "    if vid not in _pose_history:\n",
    "        _pose_history[vid] = {\"yaw\": yaw, \"pitch\": pitch, \"roll\": roll}\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "    prev = _pose_history[vid]\n",
    "    s_yaw = alpha * yaw + (1 - alpha) * prev[\"yaw\"]\n",
    "    s_pitch = alpha * pitch + (1 - alpha) * prev[\"pitch\"]\n",
    "    s_roll = alpha * roll + (1 - alpha) * prev[\"roll\"]\n",
    "    _pose_history[vid] = {\"yaw\": s_yaw, \"pitch\": s_pitch, \"roll\": s_roll}\n",
    "    return s_yaw, s_pitch, s_roll\n",
    "\n",
    "\n",
    "def cleanup_pose_history(active_vids):\n",
    "    \"\"\"Remove smoothing state for viewers no longer tracked.\"\"\"\n",
    "    stale = [v for v in _pose_history if v not in active_vids]\n",
    "    for v in stale:\n",
    "        del _pose_history[v]\n",
    "\n",
    "\n",
    "def head_pose_from_5pts(pts2d):\n",
    "    \"\"\"\n",
    "    Estimate head pose from 5 facial keypoints using solvePnP.\n",
    "    \n",
    "    FIX #2: Euler angle extraction corrected to use proper convention:\n",
    "      - Yaw   = rotation about Y-axis (head turning left/right)\n",
    "      - Pitch = rotation about X-axis (head nodding up/down)\n",
    "      - Roll  = rotation about Z-axis (head tilting sideways)\n",
    "    \n",
    "    Previous code used atan2(R[1,0], R[0,0]) for yaw, which is actually\n",
    "    Z-axis rotation, meaning left/right head turns were not measured.\n",
    "    \n",
    "    FIX #8: Rejects degenerate solvePnP solutions with |angle| > 80°,\n",
    "    which indicate the 5-point configuration is too ambiguous.\n",
    "    \n",
    "    Returns (yaw, pitch, roll) in degrees or (None, None, None) on failure.\n",
    "    \"\"\"\n",
    "    if pts2d is None:\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(\"head_pose: pts2d is None\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        if not isinstance(pts2d, np.ndarray) or pts2d.shape != (5, 2):\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(f\"head_pose: invalid shape - type={type(pts2d)} \"\n",
    "                             f\"shape={pts2d.shape if isinstance(pts2d, np.ndarray) else 'N/A'}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Validate keypoints are within frame bounds with margin\n",
    "        margin = 10\n",
    "        if (np.any(pts2d < -margin) or\n",
    "                np.any(pts2d[:, 0] > PREVIEW_WIDTH + margin) or\n",
    "                np.any(pts2d[:, 1] > PREVIEW_HEIGHT + margin)):\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(f\"head_pose: keypoints out of bounds - {pts2d}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Validate keypoints are not degenerate (all same point, etc.)\n",
    "        spread = np.max(pts2d, axis=0) - np.min(pts2d, axis=0)\n",
    "        if spread[0] < 5.0 or spread[1] < 5.0:\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(f\"head_pose: degenerate keypoints, spread={spread}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Convert to float64 for solvePnP precision\n",
    "        pts2d_f64 = pts2d.astype(np.float64)\n",
    "\n",
    "        ok, rvec, tvec = cv2.solvePnP(\n",
    "            _PTS3D, pts2d_f64, _K, _DIST,\n",
    "            flags=cv2.SOLVEPNP_ITERATIVE\n",
    "        )\n",
    "\n",
    "        if not ok:\n",
    "            _gaze_diag[\"solvepnp_fail\"] += 1\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(\"head_pose: solvePnP returned ok=False\")\n",
    "            return None, None, None\n",
    "\n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "\n",
    "        # FIX #2: Correct Euler angle extraction (Tait-Bryan angles, XYZ convention)\n",
    "        #\n",
    "        # For head pose from a camera-facing subject:\n",
    "        #   Yaw   = rotation about Y-axis = left/right head turn\n",
    "        #   Pitch = rotation about X-axis = up/down head nod\n",
    "        #   Roll  = rotation about Z-axis = sideways head tilt\n",
    "        #\n",
    "        # From rotation matrix R (ZYX decomposition):\n",
    "        #   pitch = atan2(R[2,1], R[2,2])          — X rotation\n",
    "        #   yaw   = atan2(-R[2,0], sqrt(R[2,1]²+R[2,2]²))  — Y rotation\n",
    "        #   roll  = atan2(R[1,0], R[0,0])          — Z rotation\n",
    "        #\n",
    "        # The previous code had:\n",
    "        #   yaw   = atan2(R[1,0], R[0,0])  ← This is ROLL, not yaw!\n",
    "        #   pitch = atan2(-R[2,0], sy)      ← This is actually YAW\n",
    "        # ...so yaw and roll were effectively swapped.\n",
    "\n",
    "        sy = np.sqrt(R[2, 1] ** 2 + R[2, 2] ** 2)\n",
    "\n",
    "        if sy > 1e-6:  # Not in gimbal lock\n",
    "            pitch = np.degrees(np.arctan2(R[2, 1], R[2, 2]))\n",
    "            yaw = np.degrees(np.arctan2(-R[2, 0], sy))\n",
    "            roll = np.degrees(np.arctan2(R[1, 0], R[0, 0]))\n",
    "        else:\n",
    "            # Gimbal lock fallback (extremely rare for head pose)\n",
    "            pitch = np.degrees(np.arctan2(-R[1, 2], R[1, 1]))\n",
    "            yaw = np.degrees(np.arctan2(-R[2, 0], sy))\n",
    "            roll = 0.0\n",
    "\n",
    "        # FIX #8: Reject degenerate solutions with extreme angles\n",
    "        # 5-point solvePnP can sometimes converge to physically\n",
    "        # impossible poses when landmarks are noisy\n",
    "        if abs(yaw) > 80 or abs(pitch) > 80 or abs(roll) > 80:\n",
    "            _gaze_diag[\"solvepnp_rejected\"] += 1\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(f\"head_pose: REJECTED extreme angles — \"\n",
    "                             f\"yaw={yaw:.1f}° pitch={pitch:.1f}° roll={roll:.1f}°\")\n",
    "            return None, None, None\n",
    "\n",
    "        _gaze_diag[\"solvepnp_success\"] += 1\n",
    "\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(f\"head_pose: SUCCESS — yaw={yaw:.1f}° pitch={pitch:.1f}° roll={roll:.1f}°\")\n",
    "\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "    except Exception as e:\n",
    "        _gaze_diag[\"solvepnp_fail\"] += 1\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(f\"head_pose: exception - {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def fallback_gaze_estimation(bbox):\n",
    "    \"\"\"\n",
    "    Fallback method: estimate gaze based on face position in frame.\n",
    "    Assumes face in center of frame is looking at screen.\n",
    "    \n",
    "    FIX #7: Tightened to require face within central region of frame\n",
    "    and uses stricter angle thresholds to reduce false positives.\n",
    "    Previous version would report nearly any detected face as gazing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        face_center_x = (x1 + x2) / 2.0\n",
    "        face_center_y = (y1 + y2) / 2.0\n",
    "\n",
    "        # Normalize to [-1, 1]\n",
    "        rel_x = (face_center_x - PREVIEW_WIDTH / 2) / (PREVIEW_WIDTH / 2)\n",
    "        rel_y = (face_center_y - PREVIEW_HEIGHT / 2) / (PREVIEW_HEIGHT / 2)\n",
    "\n",
    "        # FIX #7: Check if face is within the central region\n",
    "        # If face is near the edge of frame, it's likely not facing the camera\n",
    "        if abs(rel_x) > FALLBACK_CENTER_RATIO or abs(rel_y) > FALLBACK_CENTER_RATIO:\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(f\"fallback_gaze: face too far from center \"\n",
    "                             f\"(rel_x={rel_x:.2f}, rel_y={rel_y:.2f})\")\n",
    "            return None, None\n",
    "\n",
    "        # Also check face size — very small faces are likely far away/peripheral\n",
    "        face_w = x2 - x1\n",
    "        face_h = y2 - y1\n",
    "        min_face_size = min(PREVIEW_WIDTH, PREVIEW_HEIGHT) * 0.05  # At least 5% of frame\n",
    "        if face_w < min_face_size or face_h < min_face_size:\n",
    "            if GAZE_DEBUG:\n",
    "                logger.debug(f\"fallback_gaze: face too small ({face_w:.0f}x{face_h:.0f})\")\n",
    "            return None, None\n",
    "\n",
    "        # Convert to approximate angles (conservative mapping)\n",
    "        yaw = rel_x * 35.0  # Max ±35 degrees (reduced from 40)\n",
    "        pitch = rel_y * 20.0  # Max ±20 degrees (reduced from 25)\n",
    "\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(f\"fallback_gaze: center=({face_center_x:.0f},{face_center_y:.0f}) \"\n",
    "                         f\"-> yaw={yaw:.1f}° pitch={pitch:.1f}°\")\n",
    "\n",
    "        return yaw, pitch\n",
    "\n",
    "    except Exception as e:\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(f\"fallback_gaze: exception - {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def is_gazing_at_screen(pts2d, bbox=None, viewer_id=None):\n",
    "    \"\"\"\n",
    "    Determine if viewer is gazing at screen.\n",
    "    \n",
    "    Uses head pose from 5-point landmarks (primary), with fallback to\n",
    "    face-position heuristic. Applies per-viewer temporal smoothing.\n",
    "    \n",
    "    Returns (is_gazing, yaw, pitch)\n",
    "    \"\"\"\n",
    "    _gaze_diag[\"total_gaze_checks\"] += 1\n",
    "    used_fallback = False\n",
    "\n",
    "    # Track keypoint availability (FIX #5)\n",
    "    if pts2d is not None and isinstance(pts2d, np.ndarray) and pts2d.shape == (5, 2):\n",
    "        _gaze_diag[\"kpts_valid\"] += 1\n",
    "    else:\n",
    "        _gaze_diag[\"kpts_none\"] += 1\n",
    "\n",
    "    # Try primary method: head pose from 5 facial landmarks\n",
    "    yaw, pitch, roll = head_pose_from_5pts(pts2d)\n",
    "\n",
    "    # If primary failed, try fallback\n",
    "    if yaw is None and bbox is not None:\n",
    "        used_fallback = True\n",
    "        _gaze_diag[\"fallback_used\"] += 1\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(\"Using fallback gaze estimation (bbox position)\")\n",
    "        yaw, pitch = fallback_gaze_estimation(bbox)\n",
    "        roll = 0.0  # Unknown from fallback\n",
    "\n",
    "    # If both methods failed\n",
    "    if yaw is None:\n",
    "        if GAZE_DEBUG:\n",
    "            logger.debug(\"All gaze estimation methods failed\")\n",
    "        return False, None, None\n",
    "\n",
    "    # FIX #3: Apply temporal smoothing if viewer_id is provided\n",
    "    if viewer_id is not None and roll is not None:\n",
    "        yaw, pitch, roll = _smooth_pose(viewer_id, yaw, pitch, roll)\n",
    "\n",
    "    # Check if within gaze cone — use tighter thresholds for fallback\n",
    "    if used_fallback:\n",
    "        is_gazing = (abs(yaw) <= FALLBACK_MAX_YAW and abs(pitch) <= FALLBACK_MAX_PITCH)\n",
    "        if is_gazing:\n",
    "            _gaze_diag[\"fallback_gazing\"] += 1\n",
    "    else:\n",
    "        is_gazing = (abs(yaw) <= GAZE_YAW_THRESHOLD and abs(pitch) <= GAZE_PITCH_THRESHOLD)\n",
    "        if is_gazing:\n",
    "            _gaze_diag[\"primary_gazing\"] += 1\n",
    "\n",
    "    if GAZE_DEBUG:\n",
    "        method = \"FALLBACK\" if used_fallback else \"PRIMARY\"\n",
    "        status = \"✓ GAZING\" if is_gazing else \"✗ not gazing\"\n",
    "        thresholds = (f\"±{FALLBACK_MAX_YAW}°/±{FALLBACK_MAX_PITCH}°\" if used_fallback\n",
    "                      else f\"±{GAZE_YAW_THRESHOLD}°/±{GAZE_PITCH_THRESHOLD}°\")\n",
    "        logger.info(f\"{status} [{method}]: yaw={yaw:+.1f}° pitch={pitch:+.1f}° \"\n",
    "                    f\"(thresholds: {thresholds})\")\n",
    "\n",
    "    return is_gazing, yaw, pitch\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Viewer Tracker\n",
    "# ----------------------------------------------------------\n",
    "class ViewerTracker:\n",
    "    def __init__(self, iou_thr=0.3, emb_thr=0.5, timeout=5.0):\n",
    "        self.iou_thr = iou_thr\n",
    "        self.emb_thr = emb_thr\n",
    "        self.timeout = timeout\n",
    "        self.tracks = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _iou(a, b):\n",
    "        xA, yA = max(a[0], b[0]), max(a[1], b[1])\n",
    "        xB, yB = min(a[2], b[2]), min(a[3], b[3])\n",
    "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "        union = (a[2]-a[0])*(a[3]-a[1]) + (b[2]-b[0])*(b[3]-b[1]) - inter\n",
    "        return inter / union if union > 0 else 0\n",
    "\n",
    "    def _clean(self):\n",
    "        now = time.time()\n",
    "        stale = [k for k, v in self.tracks.items() if now - v['ts'] > self.timeout]\n",
    "        for k in stale:\n",
    "            del self.tracks[k]\n",
    "\n",
    "    def update(self, bboxes, embs):\n",
    "        self._clean()\n",
    "        T = list(self.tracks.keys())\n",
    "        N, M = len(T), len(bboxes)\n",
    "        now = time.time()\n",
    "\n",
    "        if N == 0:\n",
    "            out = []\n",
    "            for bb, emb in zip(bboxes, embs):\n",
    "                nid = uuid.uuid4().hex[:8]\n",
    "                self.tracks[nid] = {'bbox': bb, 'emb': emb, 'ts': now}\n",
    "                out.append((nid, True))\n",
    "            return out\n",
    "\n",
    "        cost = np.zeros((N, M), dtype=np.float32)\n",
    "        for i, tid in enumerate(T):\n",
    "            t = self.tracks[tid]\n",
    "            for j in range(M):\n",
    "                cost[i, j] = 0.4 * (1 - self._iou(t['bbox'], bboxes[j])) + 0.6 * cosine_distance(t['emb'], embs[j])\n",
    "\n",
    "        rows, cols = linear_sum_assignment(cost)\n",
    "        results = [None] * M\n",
    "\n",
    "        for r, c in zip(rows, cols):\n",
    "            tid = T[r]\n",
    "            t = self.tracks[tid]\n",
    "            if self._iou(t['bbox'], bboxes[c]) >= self.iou_thr or cosine_distance(t['emb'], embs[c]) <= self.emb_thr:\n",
    "                t.update({'bbox': bboxes[c], 'emb': embs[c], 'ts': now})\n",
    "                results[c] = (tid, False)\n",
    "\n",
    "        for j in range(M):\n",
    "            if results[j] is None:\n",
    "                nid = uuid.uuid4().hex[:8]\n",
    "                self.tracks[nid] = {'bbox': bboxes[j], 'emb': embs[j], 'ts': now}\n",
    "                results[j] = (nid, True)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Gaze Session Manager\n",
    "# ----------------------------------------------------------\n",
    "class GazeSessionManager:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}\n",
    "\n",
    "    def update(self, vid, is_gazing, ts):\n",
    "        \"\"\"\n",
    "        Update gaze session. Returns (total_gaze, continuous_gaze, is_new_gaze)\n",
    "        \"\"\"\n",
    "        if vid not in self.sessions:\n",
    "            self.sessions[vid] = {\n",
    "                'total': 0.0, 'start': None, 'prev': ts,\n",
    "                'sess_start': ts, 'last_seen': ts, 'count': 0, 'appearances': 0\n",
    "            }\n",
    "        s = self.sessions[vid]\n",
    "        s['appearances'] += 1\n",
    "        s['prev'] = ts\n",
    "        s['last_seen'] = ts\n",
    "\n",
    "        continuous = 0.0\n",
    "        is_new_gaze = False\n",
    "\n",
    "        if is_gazing:\n",
    "            if s['start'] is None:\n",
    "                s['start'] = ts\n",
    "                s['count'] += 1\n",
    "                is_new_gaze = True\n",
    "            continuous = ts - s['start']\n",
    "        else:\n",
    "            if s['start'] is not None:\n",
    "                dur = ts - s['start']\n",
    "                if dur >= MIN_GAZE_DURATION:\n",
    "                    s['total'] += dur\n",
    "                s['start'] = None\n",
    "\n",
    "        return s['total'], continuous, is_new_gaze\n",
    "\n",
    "    def end_session(self, vid):\n",
    "        \"\"\"End a session and return statistics\"\"\"\n",
    "        if vid not in self.sessions:\n",
    "            return None\n",
    "        s = self.sessions[vid]\n",
    "        now = time.time()\n",
    "\n",
    "        if s['start'] is not None:\n",
    "            dur = now - s['start']\n",
    "            if dur >= MIN_GAZE_DURATION:\n",
    "                s['total'] += dur\n",
    "\n",
    "        elapsed = now - s['sess_start']\n",
    "        raw_engagement = s['total'] / elapsed if elapsed > 0 else 0\n",
    "        capped_engagement = min(raw_engagement, 1.0)\n",
    "\n",
    "        stats = {\n",
    "            'viewer_id': vid,\n",
    "            'total_gaze_time': round(s['total'], 2),\n",
    "            'gaze_count': s['count'],\n",
    "            'session_duration': round(elapsed, 2),\n",
    "            'total_appearances': s['appearances'],\n",
    "            'engagement_rate': round(capped_engagement, 4),\n",
    "            'raw_engagement_rate': round(raw_engagement, 4)\n",
    "        }\n",
    "        del self.sessions[vid]\n",
    "        return stats\n",
    "\n",
    "    def cleanup_stale(self):\n",
    "        \"\"\"Clean up sessions that haven't been seen recently\"\"\"\n",
    "        now = time.time()\n",
    "        stale = [v for v, s in self.sessions.items()\n",
    "                 if now - s['last_seen'] > ENGAGEMENT_TIMEOUT]\n",
    "        ended = []\n",
    "        for vid in stale:\n",
    "            st = self.end_session(vid)\n",
    "            if st:\n",
    "                ended.append(st)\n",
    "        return ended\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Demographics Worker\n",
    "# ----------------------------------------------------------\n",
    "class DemographicsWorker:\n",
    "    def __init__(self, age_mdl, gender_mdl, emotion_mdl, queue_size=8):\n",
    "        self._age_mdl = age_mdl\n",
    "        self._gender_mdl = gender_mdl\n",
    "        self._emotion_mdl = emotion_mdl\n",
    "        self._queue = deque(maxlen=queue_size)\n",
    "        self._running = True\n",
    "        self._thread = threading.Thread(target=self._run, daemon=True, name=\"demo-worker\")\n",
    "        self._thread.start()\n",
    "        logger.info(f\"Demographics worker started (queue_size={queue_size})\")\n",
    "\n",
    "    def submit(self, vid, crop):\n",
    "        self._queue.append((vid, crop))\n",
    "\n",
    "    def _run(self):\n",
    "        while self._running:\n",
    "            if not self._queue:\n",
    "                time.sleep(0.05)\n",
    "                continue\n",
    "            try:\n",
    "                vid, crop = self._queue.popleft()\n",
    "                self._process(vid, crop)\n",
    "            except Exception:\n",
    "                logger.debug(f\"Demographics failed for {vid}\", exc_info=True)\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    def _process(self, vid, crop):\n",
    "        if crop is None or crop.size == 0:\n",
    "            return\n",
    "\n",
    "        age_val, gen_val, emo_val = 0, \"\", \"\"\n",
    "\n",
    "        try:\n",
    "            r = self._age_mdl.predict(crop)\n",
    "            if hasattr(r, 'results') and r.results:\n",
    "                d = r.results[0]\n",
    "                raw = d.get(\"score\", 0) if isinstance(d, dict) else getattr(d, \"score\", 0)\n",
    "                age_val = round(raw) if raw else 0\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            r = self._gender_mdl.predict(crop)\n",
    "            if hasattr(r, 'results') and r.results:\n",
    "                d = r.results[0]\n",
    "                gen_val = d.get(\"label\", \"\") if isinstance(d, dict) else getattr(d, \"label\", \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            r = self._emotion_mdl.predict(crop)\n",
    "            if hasattr(r, 'results') and r.results:\n",
    "                d = r.results[0]\n",
    "                emo_val = d.get(\"label\", \"\") if isinstance(d, dict) else getattr(d, \"label\", \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if vid in viewer_profiles:\n",
    "            if age_val > 0:\n",
    "                viewer_profiles[vid]['age'] = age_val\n",
    "            if gen_val:\n",
    "                viewer_profiles[vid]['gender'] = gen_val\n",
    "            if emo_val:\n",
    "                viewer_profiles[vid]['emotions'][emo_val] = viewer_profiles[vid]['emotions'].get(emo_val, 0) + 1\n",
    "            logger.debug(f\"Demo result: {vid} age={age_val} gender={gen_val} emotion={emo_val}\")\n",
    "\n",
    "    def stop(self):\n",
    "        self._running = False\n",
    "        self._thread.join(timeout=3.0)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Visualization\n",
    "# ----------------------------------------------------------\n",
    "def draw_overlay(img, faces):\n",
    "    for f in faces:\n",
    "        bb = f.get(\"bbox\")\n",
    "        if not bb:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, bb)\n",
    "\n",
    "        gazing = f.get(\"is_gazing\", False)\n",
    "        color = (0, 255, 0) if gazing else (0, 165, 255)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        vid = f.get(\"viewer_id\", \"?\")[:6]\n",
    "        age = f.get(\"age_est\", 0)\n",
    "        gender = f.get(\"gender\", \"\")[:1]\n",
    "        gaze_dur = f.get(\"gaze_duration\", 0.0)\n",
    "        yaw = f.get(\"yaw\")\n",
    "        pitch = f.get(\"pitch\")\n",
    "\n",
    "        label_parts = [f\"ID:{vid}\"]\n",
    "        if age > 0:\n",
    "            label_parts.append(f\"{gender}{age}\")\n",
    "        if gazing:\n",
    "            label_parts.append(f\"GAZE:{gaze_dur:.1f}s\")\n",
    "        if yaw is not None and pitch is not None:\n",
    "            label_parts.append(f\"Y{yaw:+.0f}°P{pitch:+.0f}°\")\n",
    "\n",
    "        label = \" \".join(label_parts)\n",
    "        cv2.putText(img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "        kpts = f.get(\"kpts\")\n",
    "        if kpts is not None and isinstance(kpts, np.ndarray) and kpts.shape == (5, 2):\n",
    "            for kp in kpts:\n",
    "                cv2.circle(img, tuple(map(int, kp)), 2, (255, 0, 255), -1)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Models\n",
    "# ----------------------------------------------------------\n",
    "logger.info(\"Loading models...\")\n",
    "try:\n",
    "    widerface_model = dg.load_model(\n",
    "        model_name=widerface_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded WiderFace model\")\n",
    "\n",
    "    face_embed_model = dg.load_model(\n",
    "        model_name=face_embed_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded embedding model\")\n",
    "\n",
    "    age_model = dg.load_model(\n",
    "        model_name=age_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded age model\")\n",
    "\n",
    "    gender_model = dg.load_model(\n",
    "        model_name=gender_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded gender model\")\n",
    "\n",
    "    emotion_model = dg.load_model(\n",
    "        model_name=emotion_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded emotion model\")\n",
    "\n",
    "except Exception:\n",
    "    logger.exception(\"Failed to load models\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Initialize Components\n",
    "# ----------------------------------------------------------\n",
    "camera = ThreadedCamera(fps=CAMERA_FPS, width=PREVIEW_WIDTH, height=PREVIEW_HEIGHT)\n",
    "tracker = ViewerTracker(iou_thr=0.25, emb_thr=0.5, timeout=3.0)\n",
    "gaze_mgr = GazeSessionManager()\n",
    "demo_worker = DemographicsWorker(age_model, gender_model, emotion_model, queue_size=DEMO_QUEUE_SIZE)\n",
    "\n",
    "display_available = preview_camera\n",
    "if preview_camera:\n",
    "    try:\n",
    "        cv2.namedWindow(\"test\", cv2.WINDOW_NORMAL)\n",
    "        cv2.destroyWindow(\"test\")\n",
    "        logger.info(\"Display available\")\n",
    "    except cv2.error:\n",
    "        logger.warning(\"No display — preview disabled\")\n",
    "        display_available = False\n",
    "\n",
    "fps_q = deque(maxlen=30)\n",
    "last_fps_t = time.time()\n",
    "last_log_t = time.time()\n",
    "frame_counter = 0\n",
    "total_faces_detected = 0\n",
    "total_gaze_events = 0\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"GAZE TRACKING v3.3.0-PRODUCTION STARTED\")\n",
    "logger.info(f\"  Camera: {PREVIEW_WIDTH}x{PREVIEW_HEIGHT} @ {CAMERA_FPS}fps\")\n",
    "logger.info(f\"  Focal length: {_FOCAL_LENGTH:.1f}px (Pi Camera Module 3 calibrated)\")\n",
    "logger.info(f\"  Skip frames: {SKIP_FRAMES} | Max faces: {MAX_FACES_PER_FRAME}\")\n",
    "logger.info(f\"  Min loop time: {MIN_LOOP_TIME*1000:.0f}ms\")\n",
    "logger.info(f\"  Thermal: throttle={THERMAL_THROTTLE_TEMP}°C critical={THERMAL_CRITICAL_TEMP}°C\")\n",
    "logger.info(f\"  Gaze (primary): Yaw ±{GAZE_YAW_THRESHOLD}° Pitch ±{GAZE_PITCH_THRESHOLD}° Min {MIN_GAZE_DURATION}s\")\n",
    "logger.info(f\"  Gaze (fallback): Yaw ±{FALLBACK_MAX_YAW}° Pitch ±{FALLBACK_MAX_PITCH}° Center {FALLBACK_CENTER_RATIO}\")\n",
    "logger.info(f\"  Pose smoothing: alpha={POSE_SMOOTH_ALPHA}\")\n",
    "logger.info(f\"  Gaze Debug: {'ENABLED' if GAZE_DEBUG else 'DISABLED'}\")\n",
    "logger.info(f\"  Log file: {os.path.abspath(LOG_FILE)}\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    while True:\n",
    "        loop_start = time.time()\n",
    "\n",
    "        frame = camera.read()\n",
    "        if frame is None:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        frame_counter += 1\n",
    "        current_time = time.time()\n",
    "        faces = []\n",
    "        was_processed = False\n",
    "\n",
    "        dt = current_time - last_fps_t\n",
    "        fps_q.append(1.0 / dt if dt > 0 else 0)\n",
    "        last_fps_t = current_time\n",
    "        avg_fps = sum(fps_q) / len(fps_q) if fps_q else 0\n",
    "\n",
    "        # ---- INFERENCE ----\n",
    "        if frame_counter % SKIP_FRAMES == 0:\n",
    "            cpu_temp = get_cpu_temp()\n",
    "\n",
    "            if cpu_temp >= THERMAL_CRITICAL_TEMP:\n",
    "                logger.warning(f\"CRITICAL TEMP {cpu_temp:.0f}°C — sleeping 1s\")\n",
    "                time.sleep(1.0)\n",
    "            elif cpu_temp >= THERMAL_THROTTLE_TEMP:\n",
    "                logger.info(f\"THROTTLE {cpu_temp:.0f}°C — sleeping 200ms\")\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            if cpu_temp < THERMAL_CRITICAL_TEMP:\n",
    "                try:\n",
    "                    det = widerface_model.predict(frame)\n",
    "                    was_processed = True\n",
    "\n",
    "                    if hasattr(det, 'results'):\n",
    "                        det_items = []\n",
    "                        for d in det.results:\n",
    "                            try:\n",
    "                                bbox = d.get(\"bbox\") if isinstance(d, dict) else getattr(d, \"bbox\", None)\n",
    "                                if bbox:\n",
    "                                    x1, y1, x2, y2 = bbox\n",
    "                                    det_items.append(((x2-x1)*(y2-y1), d))\n",
    "                            except Exception:\n",
    "                                continue\n",
    "                        det_items.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                        for _, d in det_items[:MAX_FACES_PER_FRAME]:\n",
    "                            try:\n",
    "                                bbox = d.get(\"bbox\") if isinstance(d, dict) else getattr(d, \"bbox\", None)\n",
    "                                x1, y1, x2, y2 = map(int, bbox)\n",
    "                                x1, y1 = max(0, x1), max(0, y1)\n",
    "                                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                                if x2 <= x1 or y2 <= y1:\n",
    "                                    continue\n",
    "\n",
    "                                crop = frame[y1:y2, x1:x2]\n",
    "                                if isinstance(d, dict):\n",
    "                                    kr = d.get(\"kpts\") or d.get(\"landmarks\") or d.get(\"keypoints\")\n",
    "                                else:\n",
    "                                    kr = getattr(d, \"kpts\", None) or getattr(d, \"landmarks\", None) or getattr(d, \"keypoints\", None)\n",
    "                                kpts = parse_keypoints(kr)\n",
    "\n",
    "                                emb_raw = {}\n",
    "                                if crop.size > 0:\n",
    "                                    try:\n",
    "                                        er = face_embed_model.predict(crop)\n",
    "                                        if hasattr(er, 'results') and er.results:\n",
    "                                            emb_raw = er.results[0]\n",
    "                                    except Exception:\n",
    "                                        pass\n",
    "\n",
    "                                _gaze_diag[\"frames_with_faces\"] += 1\n",
    "\n",
    "                                faces.append({\n",
    "                                    \"bbox\": bbox, \"kpts\": kpts, \"embedding\": emb_raw,\n",
    "                                    \"crop\": crop,\n",
    "                                    \"age_est\": 0, \"gender\": \"\", \"gender_score\": 0.0,\n",
    "                                    \"emotion\": \"\", \"emotion_score\": 0.0,\n",
    "                                })\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "                    total_faces_detected += len(faces)\n",
    "\n",
    "                except Exception:\n",
    "                    logger.exception(\"Detection failed\")\n",
    "\n",
    "        # ---- TRACKING + GAZE ----\n",
    "        if was_processed and faces:\n",
    "            bboxes = [f[\"bbox\"] for f in faces]\n",
    "            embs = [extract_embedding(f.get(\"embedding\", {})) for f in faces]\n",
    "            assignments = tracker.update(bboxes, embs)\n",
    "\n",
    "            if len(assignments) != len(faces):\n",
    "                logger.warning(f\"Track mismatch: {len(assignments)} vs {len(faces)}\")\n",
    "            else:\n",
    "                for i, f in enumerate(faces):\n",
    "                    try:\n",
    "                        vid, is_new = assignments[i]\n",
    "\n",
    "                        if is_new:\n",
    "                            crop = f.get(\"crop\")\n",
    "                            if crop is not None and crop.size > 0:\n",
    "                                demo_worker.submit(vid, crop)\n",
    "\n",
    "                        f.pop(\"crop\", None)\n",
    "\n",
    "                        kpts = f.get(\"kpts\")\n",
    "                        bbox = f.get(\"bbox\")\n",
    "\n",
    "                        # Gaze detection — now passes viewer_id for smoothing (FIX #3)\n",
    "                        if kpts is not None and isinstance(kpts, np.ndarray) and kpts.shape == (5, 2):\n",
    "                            gazing, yaw, pitch = is_gazing_at_screen(kpts, bbox, viewer_id=vid)\n",
    "                        else:\n",
    "                            gazing, yaw, pitch = is_gazing_at_screen(None, bbox, viewer_id=vid)\n",
    "\n",
    "                        total_g, cont_g, is_new_gaze = gaze_mgr.update(vid, gazing, current_time)\n",
    "\n",
    "                        f[\"is_gazing\"] = gazing\n",
    "                        f[\"gaze_duration\"] = cont_g\n",
    "                        f[\"total_gaze\"] = total_g\n",
    "                        f[\"yaw\"] = yaw\n",
    "                        f[\"pitch\"] = pitch\n",
    "                        f[\"viewer_id\"] = vid\n",
    "\n",
    "                        if vid not in viewer_profiles or is_new:\n",
    "                            viewer_profiles[vid] = {\n",
    "                                'age': 0, 'gender': '', 'first_seen': current_time, 'emotions': {}\n",
    "                            }\n",
    "\n",
    "                        prof = viewer_profiles[vid]\n",
    "                        if prof['age'] > 0:\n",
    "                            f[\"age_est\"] = prof['age']\n",
    "                        if prof['gender']:\n",
    "                            f[\"gender\"] = prof['gender']\n",
    "\n",
    "                        emo = f.get(\"emotion\", \"\")\n",
    "                        if not emo and prof.get('emotions'):\n",
    "                            emo = max(prof['emotions'].items(), key=lambda x: x[1])[0]\n",
    "                            f[\"emotion\"] = emo\n",
    "\n",
    "                        if is_new_gaze:\n",
    "                            total_gaze_events += 1\n",
    "                            log_gaze_event({\n",
    "                                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                                'event': 'gaze_start',\n",
    "                                'viewer_id': vid,\n",
    "                                'demographics': {'age': prof['age'], 'gender': prof['gender'], 'emotion': emo},\n",
    "                                'head_pose': {\n",
    "                                    'yaw': round(yaw, 2) if yaw is not None else None,\n",
    "                                    'pitch': round(pitch, 2) if pitch is not None else None\n",
    "                                }\n",
    "                            })\n",
    "                            logger.info(f\"GAZE_START: {vid} ({prof['gender']} {prof['age']}y)\")\n",
    "\n",
    "                    except Exception:\n",
    "                        logger.debug(f\"Face {i} error\", exc_info=True)\n",
    "                        continue\n",
    "\n",
    "        # ---- HEARTBEAT ----\n",
    "        if current_time - last_log_t >= LOG_INTERVAL:\n",
    "            active = []\n",
    "            for vid, s in gaze_mgr.sessions.items():\n",
    "                if s['start'] is not None:\n",
    "                    ct = current_time - s['start']\n",
    "                    if ct >= MIN_GAZE_DURATION:\n",
    "                        p = viewer_profiles.get(vid, {})\n",
    "                        emos = p.get('emotions', {})\n",
    "                        active.append({\n",
    "                            'viewer_id': vid,\n",
    "                            'continuous_gaze': round(ct, 1),\n",
    "                            'total_gaze': round(s['total'], 1),\n",
    "                            'demographics': {\n",
    "                                'age': p.get('age', 0),\n",
    "                                'gender': p.get('gender', ''),\n",
    "                                'emotion': max(emos.items(), key=lambda x: x[1])[0] if emos else ''\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "            env = read_bme688_data()\n",
    "            noise_db = read_noise_level_db()\n",
    "            if noise_db is not None:\n",
    "                env[\"noise_db\"] = noise_db\n",
    "\n",
    "            # FIX #5: Include gaze diagnostics in heartbeat\n",
    "            gaze_diag = get_gaze_diagnostics()\n",
    "\n",
    "            heartbeat = {\n",
    "                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                'event': 'heartbeat',\n",
    "                'active_gazers': len(active),\n",
    "                'tracked_viewers': len(gaze_mgr.sessions),\n",
    "                'total_faces_detected': total_faces_detected,\n",
    "                'total_gaze_events': total_gaze_events,\n",
    "                'fps': round(avg_fps, 1),\n",
    "                'cpu_temp': round(get_cpu_temp(), 1),\n",
    "                'environment': env,\n",
    "                'gaze_diagnostics': gaze_diag\n",
    "            }\n",
    "            if active:\n",
    "                heartbeat['gazers'] = active\n",
    "\n",
    "            log_gaze_event(heartbeat)\n",
    "            logger.info(f\"HEARTBEAT: gazers={len(active)} tracked={len(gaze_mgr.sessions)} \"\n",
    "                       f\"faces_total={total_faces_detected} gaze_events={total_gaze_events} \"\n",
    "                       f\"FPS={avg_fps:.1f} CPU={get_cpu_temp():.0f}°C \"\n",
    "                       f\"kpts_valid={gaze_diag['kpts_valid_pct']}% \"\n",
    "                       f\"pnp_ok={gaze_diag['solvepnp_success_pct']}% \"\n",
    "                       f\"fallback={gaze_diag['fallback_pct']}%\")\n",
    "\n",
    "            # Periodically clean up pose smoothing state\n",
    "            active_vids = set(gaze_mgr.sessions.keys())\n",
    "            cleanup_pose_history(active_vids)\n",
    "\n",
    "            last_log_t = current_time\n",
    "\n",
    "        # ---- CLEANUP STALE SESSIONS ----\n",
    "        for ss in gaze_mgr.cleanup_stale():\n",
    "            vid = ss['viewer_id']\n",
    "            p = viewer_profiles.get(vid, {})\n",
    "            log_gaze_event({\n",
    "                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                'event': 'session_end',\n",
    "                'viewer_id': vid,\n",
    "                'session_stats': ss,\n",
    "                'demographics': {\n",
    "                    'age': p.get('age', 0),\n",
    "                    'gender': p.get('gender', ''),\n",
    "                    'emotions': p.get('emotions', {})\n",
    "                },\n",
    "                'environment': read_bme688_data()\n",
    "            })\n",
    "            logger.info(f\"SESSION END: {vid} gaze={ss['total_gaze_time']:.1f}s \"\n",
    "                       f\"dur={ss['session_duration']:.1f}s \"\n",
    "                       f\"engage={ss['engagement_rate']:.1%}\")\n",
    "            viewer_profiles.pop(vid, None)\n",
    "\n",
    "        # ---- PREVIEW ----\n",
    "        if display_available:\n",
    "            disp = frame.copy()\n",
    "            if faces:\n",
    "                disp = draw_overlay(disp, faces)\n",
    "            n_gaze = sum(1 for f in faces if f.get(\"is_gazing\"))\n",
    "            t = get_cpu_temp()\n",
    "            cv2.putText(disp, f\"FPS:{avg_fps:.1f} F:{len(faces)} G:{n_gaze} T:{t:.0f}C\",\n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Audience Gaze Analysis\", disp)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                logger.info(\"User exit\")\n",
    "                break\n",
    "\n",
    "        # ---- ENFORCE MIN LOOP TIME ----\n",
    "        elapsed = time.time() - loop_start\n",
    "        if elapsed < MIN_LOOP_TIME:\n",
    "            time.sleep(MIN_LOOP_TIME - elapsed)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Interrupted\")\n",
    "finally:\n",
    "    logger.info(\"Shutting down...\")\n",
    "\n",
    "    # Log final gaze diagnostics\n",
    "    final_diag = get_gaze_diagnostics()\n",
    "    logger.info(f\"FINAL GAZE DIAGNOSTICS: {json.dumps(final_diag)}\")\n",
    "\n",
    "    for vid in list(gaze_mgr.sessions.keys()):\n",
    "        st = gaze_mgr.end_session(vid)\n",
    "        if st:\n",
    "            p = viewer_profiles.get(vid, {})\n",
    "            log_gaze_event({\n",
    "                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                'event': 'shutdown_session_end',\n",
    "                'viewer_id': vid,\n",
    "                'session_stats': st,\n",
    "                'demographics': {\n",
    "                    'age': p.get('age', 0),\n",
    "                    'gender': p.get('gender', ''),\n",
    "                    'emotions': p.get('emotions', {})\n",
    "                }\n",
    "            })\n",
    "            logger.info(f\"Final: {vid} gaze={st['total_gaze_time']:.1f}s dur={st['session_duration']:.1f}s engage={st['engagement_rate']:.1%}\")\n",
    "\n",
    "    demo_worker.stop()\n",
    "    camera.stop()\n",
    "    if display_available:\n",
    "        cv2.destroyAllWindows()\n",
    "    logger.info(f\"Clean shutdown. Total faces={total_faces_detected} gaze_events={total_gaze_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6943155-d595-4c9d-ac63-e5d2a98dad54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63427df7-e0c5-4fc4-a1ca-b644ae895620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audience-analysis_env)",
   "language": "python",
   "name": "audience-analysis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
