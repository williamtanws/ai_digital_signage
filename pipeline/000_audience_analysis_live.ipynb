{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DegirumException",
     "evalue": "Model 'tddfa_mobilenet_v1--120x120_quant_hailort_hailo8_1' is not found in model zoo '../models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDegirumException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 448\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;66;03m# Load Emotion Recognition Model\u001b[39;00m\n\u001b[32m    440\u001b[39m emotion_model = dg.load_model(\n\u001b[32m    441\u001b[39m     model_name=emotion_model_name,\n\u001b[32m    442\u001b[39m     inference_host_address=inference_host_address,\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m     device_type=device_type\n\u001b[32m    446\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m tddfa_model = \u001b[43mdg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtddfa_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43minference_host_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_host_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzoo_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzoo_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_type\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m face_emotion_model  = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, emotion_model)\n\u001b[32m    457\u001b[39m face_age_model      = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, age_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/__init__.py:222\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name, inference_host_address, zoo_url, token, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03mLoad a model from the model zoo for the inference.\u001b[39;00m\n\u001b[32m    200\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m \u001b[33;03m    An instance of [degirum.model.Model][] model handling object to be used for AI inferences.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m zoo = ZooManager(inference_host_address, zoo_url, token)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzoo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/log.py:84\u001b[39m, in \u001b[36mlog_wrap.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     83\u001b[39m     logger.log(log_level, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     86\u001b[39m     t2 = time.time_ns()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/zoo_manager.py:319\u001b[39m, in \u001b[36mZooManager.load_model\u001b[39m\u001b[34m(self, model_name, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;129m@log_wrap\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> Model:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create and return the model handling object for given model name.\u001b[39;00m\n\u001b[32m    260\u001b[39m \n\u001b[32m    261\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_zoo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(Model, key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/log.py:84\u001b[39m, in \u001b[36mlog_wrap.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     83\u001b[39m     logger.log(log_level, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     86\u001b[39m     t2 = time.time_ns()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/_zoo_accessor.py:314\u001b[39m, in \u001b[36m_LocalInferenceLocalDirZooAccessor.load_model\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;129m@log_wrap\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    306\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create model object for given model identifier.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m    308\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m \u001b[33;03m    Returns model object corresponding to model identifier.\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     model_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# Check Supported Device Types for this model\u001b[39;00m\n\u001b[32m    317\u001b[39m     supported_device_types = \u001b[38;5;28mself\u001b[39m._model_supported_device_types(model_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/log.py:84\u001b[39m, in \u001b[36mlog_wrap.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     83\u001b[39m     logger.log(log_level, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     86\u001b[39m     t2 = time.time_ns()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_digital_signage/ai_digital_signage_env/lib/python3.11/site-packages/degirum/_zoo_accessor.py:132\u001b[39m, in \u001b[36m_CommonZooAccessor.model_info\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m copy.deepcopy(model_info)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DegirumException(\n\u001b[32m    133\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not found in model zoo \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[31mDegirumException\u001b[39m: Model 'tddfa_mobilenet_v1--120x120_quant_hailort_hailo8_1' is not found in model zoo '../models'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Audience Analysis Script using Hailo-8, DeGirum SDK, and PiCamera2\n",
    "# ----------------------------------------------------------\n",
    "# Models: Face Detection, Age, Gender, Emotion, Embedding\n",
    "# Hardware: Raspberry Pi 5 + Hailo-8 + Camera Module 3\n",
    "# Filename: 000_audience_analysis_live.ipynb\n",
    "# Created date: 01 July 2025\n",
    "# Last modified date: 06 July 2025\n",
    "# Version: 1.0.0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Sample JSON Output (Single Viewer Record)\n",
    "# ----------------------------------------------------------\n",
    "# {\n",
    "#   \"timestamp\": \"2025-07-13T03:42:46.614502Z\",\n",
    "#   \"device\": {\n",
    "#     \"mac_address\": \"88:A2:9E:1C:49:6F\",\n",
    "#     \"hailo_temp_c\": 51.58,\n",
    "#     \"coordinates\": \"3.1319N, 101.6841E\",\n",
    "#     \"cpu_temp_c\": 56.75,\n",
    "#     \"cpu_usage_percent\": 2.0,\n",
    "#     \"ram_used_mb\": 2055.5,\n",
    "#     \"ram_total_mb\": 16219.31,\n",
    "#     \"disk_used_gb\": 9.36,\n",
    "#     \"disk_total_gb\": 113.76\n",
    "#   },\n",
    "#   \"env\": {\n",
    "#     \"temp_c\": 29.24,\n",
    "#     \"humidity\": 52.19,\n",
    "#     \"pressure_hPa\": 1004.3,\n",
    "#     \"gas_resistance_ohms\": 102400000.0\n",
    "#   },\n",
    "#   \"viewer_id\": \"52419e40\",\n",
    "#   \"summary\": {\n",
    "#     \"appearance_count\": 6,\n",
    "#     \"avg_age_est\": 24.17,\n",
    "#     \"avg_age_score\": 24.07,\n",
    "#     \"avg_gender_score\": 0.87,\n",
    "#     \"majority_gender\": \"Female\",\n",
    "#     \"avg_emotion_score\": 0.95,\n",
    "#     \"majority_emotion\": \"neutral\",\n",
    "#     \"avg_attention_duration\": 5.4,\n",
    "#     \"gaze_at_screen_ratio\": 0.33\n",
    "#   }\n",
    "# }\n",
    "# \n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import degirum as dg\n",
    "import degirum_tools\n",
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "from datetime import datetime\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import bme680\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from hailo_platform import Device\n",
    "import psutil\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "def handle_uncaught_exceptions(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "    logger.critical(\"Uncaught Exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "\n",
    "sys.excepthook = handle_uncaught_exceptions\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------------------------\n",
    "preview_camera = False   # set True to see overlays on screen\n",
    "console_output  = False  # set True to also log to console\n",
    "\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url               = \"../models\"\n",
    "token                 = \"\"\n",
    "device_type           = \"HAILORT/HAILO8\"\n",
    "\n",
    "face_det_model_name   = \"retinaface_mobilenet--736x1280_quant_hailort_hailo8_1\"\n",
    "face_embed_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8_1\"\n",
    "age_model_name        = \"yolov8n_relu6_age--256x256_quant_hailort_hailo8_1\"\n",
    "gender_model_name     = \"yolov8n_relu6_fairface_gender--256x256_quant_hailort_hailo8_1\"\n",
    "emotion_model_name    = \"emotion_recognition_fer2013--64x64_quant_hailort_multidevice_1\"\n",
    "# TDFFA model (for head pose / gaze estimation)\n",
    "tddfa_model_name = \"tddfa_mobilenet_v1--120x120_quant_hailort_hailo8_1\"\n",
    "\n",
    "EMB_DIM = 128  # adjust if your embedding is larger\n",
    "SUMMARY_TIMEOUT_SEC = 10  # seconds without detection to log summary\n",
    "viewer_summaries = {}     # holds stats per viewer\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Logging Setup\n",
    "# ----------------------------------------------------------\n",
    "os.makedirs(\"../logs\", exist_ok=True)\n",
    "logger = logging.getLogger(\"audience_analysis_live\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "\n",
    "handler = TimedRotatingFileHandler(\n",
    "    \"../logs/audience_analysis_live.log\",\n",
    "    when=\"H\", interval=1, backupCount=4, utc=True\n",
    ")\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "if console_output:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(handler.formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# BME688 Setup\n",
    "# ----------------------------------------------------------\n",
    "def set_bme688_sensor(sensor):\n",
    "    sensor.set_humidity_oversample(bme680.OS_2X)\n",
    "    sensor.set_pressure_oversample(bme680.OS_4X)\n",
    "    sensor.set_temperature_oversample(bme680.OS_8X)\n",
    "    sensor.set_filter(bme680.FILTER_SIZE_3)\n",
    "    sensor.set_gas_status(bme680.ENABLE_GAS_MEAS)\n",
    "\n",
    "try:\n",
    "    bme_sensor = bme680.BME680(bme680.I2C_ADDR_PRIMARY)\n",
    "    set_bme688_sensor(bme_sensor)\n",
    "except (RuntimeError, IOError):\n",
    "    bme_sensor = bme680.BME680(bme680.I2C_ADDR_SECONDARY)\n",
    "    set_bme688_sensor(bme_sensor)\n",
    "\n",
    "def read_bme688_data():\n",
    "    if bme_sensor.get_sensor_data():\n",
    "        return {\n",
    "            \"temp_c\": round(bme_sensor.data.temperature, 2),\n",
    "            \"humidity\": round(bme_sensor.data.humidity, 2),\n",
    "            \"pressure_hPa\": round(bme_sensor.data.pressure, 2),\n",
    "            \"gas_resistance_ohms\": round(bme_sensor.data.gas_resistance, 2)\n",
    "        }\n",
    "    return {\"temp_c\": None, \"humidity\": None, \"pressure_hPa\": None, \"gas_resistance_ohms\": None}\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Camera Streaming Class\n",
    "# ----------------------------------------------------------\n",
    "class CameraStream:\n",
    "    def __init__(self, fps=5):\n",
    "        self.picam2 = Picamera2()\n",
    "        self.interval = 1.0 / fps\n",
    "        self.picam2.configure(self.picam2.create_preview_configuration(\n",
    "            main={\"format\": \"RGB888\"}\n",
    "        ))\n",
    "        self.picam2.start(show_preview=False)\n",
    "        time.sleep(2)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            start = time.time()\n",
    "            frame = self.picam2.capture_array()\n",
    "            yield frame\n",
    "            elapsed = time.time() - start\n",
    "            time.sleep(max(0, self.interval - elapsed))\n",
    "\n",
    "    def stop(self):\n",
    "        self.picam2.stop()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------------------------------------\n",
    "def cosine_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"1 - cosine similarity\"\"\"\n",
    "    num = np.dot(a, b)\n",
    "    den = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return 1.0 - (num / den) if den > 0 else 1.0\n",
    "\n",
    "def draw_overlay(image, emo_res, age_res, gen_res):\n",
    "    for i, r in enumerate(emo_res):\n",
    "        try:\n",
    "            x1, y1, x2, y2 = map(int, r.get(\"bbox\", []))\n",
    "            age   = round(age_res[i].get(\"score\", 0))\n",
    "            g_lbl = gen_res[i].get(\"label\", \"\")\n",
    "            g_sc  = gen_res[i].get(\"score\", 0.0)\n",
    "            emo   = emo_res[i].get(\"label\", \"\")\n",
    "            e_sc  = emo_res[i].get(\"score\", 0.0)\n",
    "\n",
    "            label = f\"{g_lbl} ({g_sc:.2f}) | Age: {age} | {emo} ({e_sc:.2f})\"\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0,255,255), 2)\n",
    "            (w,h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)\n",
    "            cv2.rectangle(image, (x1,y1-22),(x1+w,y1),(0,255,255),-1)\n",
    "            cv2.putText(image, label, (x1,y1-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0,0,0),1)\n",
    "        except:\n",
    "            pass\n",
    "    return image\n",
    "\n",
    "def get_mac_address():\n",
    "    mac = uuid.getnode()\n",
    "    return \":\".join(f\"{(mac>>i)&0xff:02x}\" for i in range(40,-1,-8)).upper()\n",
    "\n",
    "mac_address = get_mac_address()\n",
    "\n",
    "# Raspberry Pi system metrics\n",
    "def get_rpi_system_metrics():\n",
    "    import psutil, shutil\n",
    "\n",
    "    # CPU temperature\n",
    "    try:\n",
    "        with open(\"/sys/class/thermal/thermal_zone0/temp\", \"r\") as f:\n",
    "            cpu_temp = int(f.read().strip()) / 1000.0\n",
    "    except FileNotFoundError:\n",
    "        cpu_temp = None\n",
    "\n",
    "    # CPU usage (sampled over 0.5s)\n",
    "    cpu_usage = psutil.cpu_percent(interval=0.5)\n",
    "\n",
    "    # RAM\n",
    "    mem = psutil.virtual_memory()\n",
    "    ram_total_mb = round(mem.total / (1024 * 1024), 2)\n",
    "    ram_used_mb = round(mem.used / (1024 * 1024), 2)\n",
    "\n",
    "    # Disk\n",
    "    disk = shutil.disk_usage(\"/\")\n",
    "    disk_total_gb = round(disk.total / (1024 * 1024 * 1024), 2)\n",
    "    disk_used_gb = round(disk.used / (1024 * 1024 * 1024), 2)\n",
    "\n",
    "    return {\n",
    "        \"cpu_temp_c\": cpu_temp,\n",
    "        \"cpu_usage_percent\": cpu_usage,\n",
    "        \"ram_used_mb\": ram_used_mb,\n",
    "        \"ram_total_mb\": ram_total_mb,\n",
    "        \"disk_used_gb\": disk_used_gb,\n",
    "        \"disk_total_gb\": disk_total_gb\n",
    "    }\n",
    "\n",
    "# Get Hailo-8 temperature(s)\n",
    "def get_hailo_temp():\n",
    "    temps = []\n",
    "    devices = [Device(di) for di in Device.scan()]\n",
    "    for dev in devices:\n",
    "        temp_info = dev.control.get_chip_temperature()\n",
    "        # Safely get only known attribute(s)\n",
    "        # temps.append({\n",
    "        #     \"device_id\": dev.device_id,\n",
    "        #     \"ts0_temp\": temp_info.ts0_temperature\n",
    "        # })\n",
    "        return round(temp_info.ts0_temperature, 2)\n",
    "    return temps\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Run Inference Generator\n",
    "# ----------------------------------------------------------\n",
    "def run_inference(video_source):\n",
    "    for frame in video_source:\n",
    "        yield {\n",
    "            \"emotion\": face_emotion_model.predict(frame),\n",
    "            \"age\":     face_age_model.predict(frame),\n",
    "            \"gender\":  face_gender_model.predict(frame),\n",
    "            \"embedding\": face_embed_model_comp.predict(frame)\n",
    "        }\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ViewerTracker: IoU + Embedding Matching\n",
    "# ----------------------------------------------------------\n",
    "class ViewerTracker:\n",
    "    def __init__(self, iou_threshold=0.3, emb_threshold=0.4,\n",
    "                 w_iou=0.5, w_emb=0.5, timeout_sec=10):\n",
    "        self.iou_thr = iou_threshold\n",
    "        self.emb_thr = emb_threshold\n",
    "        self.w_iou   = w_iou\n",
    "        self.w_emb   = w_emb\n",
    "        self.timeout = timeout_sec\n",
    "        self.tracks  = {}  # id -> {'bbox', 'emb', 'last_seen'}\n",
    "\n",
    "    @staticmethod\n",
    "    def _iou(a, b):\n",
    "        xA, yA = max(a[0],b[0]), max(a[1],b[1])\n",
    "        xB, yB = min(a[2],b[2]), min(a[3],b[3])\n",
    "        inter = max(0, xB-xA) * max(0, yB-yA)\n",
    "        areaA = (a[2]-a[0])*(a[3]-a[1])\n",
    "        areaB = (b[2]-b[0])*(b[3]-b[1])\n",
    "        uni   = areaA + areaB - inter\n",
    "        return inter/uni if uni>0 else 0\n",
    "\n",
    "    def _clean(self):\n",
    "        now = time.time()\n",
    "        for tid in list(self.tracks):\n",
    "            if now - self.tracks[tid]['last_seen'] > self.timeout:\n",
    "                del self.tracks[tid]\n",
    "\n",
    "    def update(self, det_bboxes, det_embs):\n",
    "        self._clean()\n",
    "        T = list(self.tracks.keys())\n",
    "        N, M = len(T), len(det_bboxes)\n",
    "\n",
    "        # no existing tracks → all new\n",
    "        if N == 0:\n",
    "            out = []\n",
    "            for bb, emb in zip(det_bboxes, det_embs):\n",
    "                nid = uuid.uuid4().hex[:8]\n",
    "                self.tracks[nid] = {'bbox': bb, 'emb': emb,\n",
    "                                     'last_seen': time.time()}\n",
    "                out.append((nid, True))\n",
    "            return out\n",
    "\n",
    "        # build cost matrix\n",
    "        cost = np.zeros((N, M), dtype=np.float32)\n",
    "        for i, tid in enumerate(T):\n",
    "            tb = self.tracks[tid]['bbox']\n",
    "            te = self.tracks[tid]['emb']\n",
    "            for j, (db, de) in enumerate(zip(det_bboxes, det_embs)):\n",
    "                iou_score = self._iou(tb, db)\n",
    "                emb_dist  = cosine_distance(te, de)\n",
    "                cost[i,j] = self.w_iou*(1-iou_score) + self.w_emb*emb_dist\n",
    "\n",
    "        # Hungarian assignment\n",
    "        rows, cols = linear_sum_assignment(cost)\n",
    "        results = [None]*M\n",
    "\n",
    "        # accept matches under one of the thresholds\n",
    "        for r, c in zip(rows, cols):\n",
    "            tid = T[r]\n",
    "            iou_score = self._iou(self.tracks[tid]['bbox'], det_bboxes[c])\n",
    "            emb_dist  = cosine_distance(self.tracks[tid]['emb'], det_embs[c])\n",
    "            logger.debug(f\"[Tracker] comparing track {tid} ? det {c}: IoU={iou_score:.2f}, emb_dist={emb_dist:.2f}\")\n",
    "            if iou_score >= self.iou_thr or emb_dist <= self.emb_thr:\n",
    "                self.tracks[tid].update({\n",
    "                    'bbox': det_bboxes[c],\n",
    "                    'emb':  det_embs[c],\n",
    "                    'last_seen': time.time()\n",
    "                })\n",
    "                results[c] = (tid, False)\n",
    "            else:\n",
    "                logger.debug(f\"[Tracker] rejecting match (IoU<{self.iou_thr} AND emb_dist>{self.emb_thr})\")\n",
    "\n",
    "        # unmatched → new\n",
    "        for j in range(M):\n",
    "            if results[j] is None:\n",
    "                nid = uuid.uuid4().hex[:8]\n",
    "                self.tracks[nid] = {\n",
    "                    'bbox': det_bboxes[j],\n",
    "                    'emb':  det_embs[j],\n",
    "                    'last_seen': time.time()\n",
    "                }\n",
    "                results[j] = (nid, True)\n",
    "\n",
    "        return results\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Gaze\n",
    "# ----------------------------------------------------------\n",
    "def estimate_gaze_from_tddfa(frame, bbox, model, yaw_thresh=25, pitch_thresh=20):\n",
    "    \"\"\"\n",
    "    Estimate if the viewer is looking at the screen based on head pose from TDFFA.\n",
    "\n",
    "    Args:\n",
    "        frame (np.ndarray): Full RGB frame\n",
    "        bbox (list[int]): [x1, y1, x2, y2] face bounding box\n",
    "        model (DeGirumModel): Loaded TDFFA model via DeGirum SDK\n",
    "        yaw_thresh (float): Max yaw (degrees) to consider facing screen\n",
    "        pitch_thresh (float): Max pitch (degrees) to consider facing screen\n",
    "\n",
    "    Returns:\n",
    "        bool: True if looking at screen\n",
    "        float: yaw angle\n",
    "        float: pitch angle\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        face_crop = frame[y1:y2, x1:x2]\n",
    "        if face_crop.shape[0] < 10 or face_crop.shape[1] < 10:\n",
    "            return False, None, None\n",
    "\n",
    "        resized = cv2.resize(face_crop, (120, 120))  # model expects 120x120\n",
    "        result = model(resized)\n",
    "\n",
    "        # Assume model output: [yaw, pitch, roll, ...]\n",
    "        pose_data = result.results[0].get(\"data\", [])\n",
    "        if not pose_data or len(pose_data[0]) < 3:\n",
    "            return False, None, None\n",
    "            \n",
    "        yaw = pose_data[0]\n",
    "        pitch = pose_data[1]\n",
    "\n",
    "        is_looking = abs(yaw) <= yaw_thresh and abs(pitch) <= pitch_thresh\n",
    "        return is_looking, yaw, pitch\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"[Gaze] Error estimating gaze: {e}\")\n",
    "        return False, None, None\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Models & Compound Pipelines\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Load Face Detection Model\n",
    "    face_det_model = dg.load_model(\n",
    "        model_name=face_det_model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    face_det_model.overlay_color = [(255, 255, 0), (0, 255, 0)]\n",
    "    \n",
    "    # Load Face Embedding Model\n",
    "    face_embed_model = dg.load_model(\n",
    "        model_name=face_embed_model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    \n",
    "    # Load Age Estimation Model\n",
    "    age_model = dg.load_model(\n",
    "        model_name=age_model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    \n",
    "    # Load Gender Classification Model\n",
    "    gender_model = dg.load_model(\n",
    "        model_name=gender_model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    \n",
    "    # Load Emotion Recognition Model\n",
    "    emotion_model = dg.load_model(\n",
    "        model_name=emotion_model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token,\n",
    "        device_type=device_type\n",
    "    )\n",
    "\n",
    "    tddfa_model = dg.load_model(\n",
    "        model_name=tddfa_model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    \n",
    "    face_emotion_model  = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, emotion_model)\n",
    "    face_age_model      = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, age_model)\n",
    "    face_gender_model   = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, gender_model)\n",
    "    face_embed_model_comp = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, face_embed_model)\n",
    "except Exception:\n",
    "    logger.exception(\"Failed to load model\")\n",
    "    raise\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Main Runtime\n",
    "# ----------------------------------------------------------\n",
    "video_source = CameraStream(fps=5)\n",
    "tracker = ViewerTracker(\n",
    "    iou_threshold=0.2,    # a little lower to catch small overlaps\n",
    "    emb_threshold=0.6,    # since embeddings are normalized, distances ? [0,2]\n",
    "    w_iou=0.5,\n",
    "    w_emb=0.5,\n",
    "    timeout_sec=10\n",
    ")\n",
    "\n",
    "try:\n",
    "    with degirum_tools.Display(\"Audience Analysis\") as disp:\n",
    "        try:\n",
    "            for results in run_inference(video_source):\n",
    "                emo_res = results[\"emotion\"].results\n",
    "                age_res = results[\"age\"].results\n",
    "                gen_res = results[\"gender\"].results\n",
    "                emb_res = results[\"embedding\"].results\n",
    "    \n",
    "                if not emo_res:\n",
    "                    continue\n",
    "    \n",
    "                bboxes = [r[\"bbox\"] for r in emo_res]\n",
    "                embs   = [\n",
    "                    np.array(r.get(\"data\", [np.zeros(EMB_DIM)])[0], dtype=np.float32)\n",
    "                    for r in emb_res\n",
    "                ]\n",
    "    \n",
    "                assignments = tracker.update(bboxes, embs)\n",
    "    \n",
    "                frame    = results[\"emotion\"].image\n",
    "                env_data = read_bme688_data()\n",
    "                min_len  = min(len(emo_res), len(age_res), len(gen_res), len(embs))\n",
    "    \n",
    "                if preview_camera:\n",
    "                    frame = draw_overlay(frame, emo_res, age_res, gen_res)\n",
    "                    disp.show(frame)\n",
    "    \n",
    "                # for i in range(min_len):\n",
    "                #     vid, is_new = assignments[i]\n",
    "                #     out = {\n",
    "                #         \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                #         \"device\": {\n",
    "                #             \"mac_address\": mac_address,\n",
    "                #             \"cpu_temp_c\": get_rpi_cpu_temp(),\n",
    "                #             \"hailo_temp_c\": get_hailo_temp(),\n",
    "                #             \"coordinates\": \"3.1319N, 101.6841E\"\n",
    "                #         },\n",
    "                #         \"env\": env_data,\n",
    "                #         \"viewer_id\": vid,\n",
    "                #         \"is_new_viewer\": is_new,\n",
    "                #         \"age_est\": round(age_res[i].get(\"score\", 0)),\n",
    "                #         \"age_score\": age_res[i].get(\"score\", 0.0),\n",
    "                #         \"gender\": gen_res[i].get(\"label\", \"\"),\n",
    "                #         \"gender_score\": gen_res[i].get(\"score\", 0.0),\n",
    "                #         \"emotion\": emo_res[i].get(\"label\", \"\"),\n",
    "                #         \"emotion_score\": emo_res[i].get(\"score\", 0.0),\n",
    "                #         \"attention_duration\": round(random.uniform(2.0, 7.5), 1),\n",
    "                #         \"gaze_at_screen\": random.choice([True, False])\n",
    "                #     }\n",
    "                #     logger.info(json.dumps(out))\n",
    "                now = time.time()\n",
    "    \n",
    "                for i in range(min_len):\n",
    "                    vid, is_new = assignments[i]\n",
    "                    age_est = round(age_res[i].get(\"score\", 0))\n",
    "                    age_score = age_res[i].get(\"score\", 0.0)\n",
    "                    gender = gen_res[i].get(\"label\", \"\")\n",
    "                    gender_score = gen_res[i].get(\"score\", 0.0)\n",
    "                    emotion = emo_res[i].get(\"label\", \"\")\n",
    "                    emotion_score = emo_res[i].get(\"score\", 0.0)\n",
    "                    attention_duration = round(random.uniform(2.0, 7.5), 1)\n",
    "                    # gaze = random.choice([True, False])\n",
    "                    bbox = bboxes[i]\n",
    "                    gaze, yaw, pitch = estimate_gaze_from_tddfa(frame, bbox, tddfa_model)\n",
    "                    logger.debug(f\"[Gaze] Viewer {vid} | Yaw: {yaw:.2f}, Pitch: {pitch:.2f} | Looking: {gaze}\")\n",
    "                \n",
    "                    if vid not in viewer_summaries:\n",
    "                        viewer_summaries[vid] = {\n",
    "                            \"viewer_id\": vid,\n",
    "                            \"count\": 0,\n",
    "                            \"age_sum\": 0.0,\n",
    "                            \"age_score_sum\": 0.0,\n",
    "                            \"gender_score_sum\": 0.0,\n",
    "                            \"emotion_score_sum\": 0.0,\n",
    "                            \"attention_duration_sum\": 0.0,\n",
    "                            \"gaze_count\": 0,\n",
    "                            \"gender_votes\": {},\n",
    "                            \"emotion_votes\": {},\n",
    "                            \"last_seen\": now\n",
    "                        }\n",
    "                \n",
    "                    stats = viewer_summaries[vid]\n",
    "                    stats[\"count\"] += 1\n",
    "                    stats[\"age_sum\"] += age_est\n",
    "                    stats[\"age_score_sum\"] += age_score\n",
    "                    stats[\"gender_score_sum\"] += gender_score\n",
    "                    stats[\"emotion_score_sum\"] += emotion_score\n",
    "                    stats[\"attention_duration_sum\"] += attention_duration\n",
    "                    stats[\"gaze_count\"] += int(gaze)\n",
    "                    stats[\"last_seen\"] = now\n",
    "                \n",
    "                    # Vote count\n",
    "                    stats[\"gender_votes\"][gender] = stats[\"gender_votes\"].get(gender, 0) + 1\n",
    "                    stats[\"emotion_votes\"][emotion] = stats[\"emotion_votes\"].get(emotion, 0) + 1\n",
    "                \n",
    "                # -------- Timeout Summary Logging --------\n",
    "                expired_viewers = []\n",
    "                for vid, stats in viewer_summaries.items():\n",
    "                    if now - stats[\"last_seen\"] >= SUMMARY_TIMEOUT_SEC:\n",
    "                        majority_gender = max(stats[\"gender_votes\"], key=stats[\"gender_votes\"].get, default=None)\n",
    "                        majority_emotion = max(stats[\"emotion_votes\"], key=stats[\"emotion_votes\"].get, default=None)\n",
    "                \n",
    "                        avg_age = round(stats[\"age_sum\"] / stats[\"count\"], 2)\n",
    "                        avg_age_score = round(stats[\"age_score_sum\"] / stats[\"count\"], 2)\n",
    "                        avg_gender_score = round(stats[\"gender_score_sum\"] / stats[\"count\"], 2)\n",
    "                        avg_emotion_score = round(stats[\"emotion_score_sum\"] / stats[\"count\"], 2)\n",
    "                        avg_attention = round(stats[\"attention_duration_sum\"] / stats[\"count\"], 1)\n",
    "                        gaze_ratio = round(stats[\"gaze_count\"] / stats[\"count\"], 2)\n",
    "                \n",
    "                        summary_out = {\n",
    "                            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                            \"device\": {\n",
    "                                \"mac_address\": mac_address,\n",
    "                                \"hailo_temp_c\": get_hailo_temp(),\n",
    "                                \"coordinates\": \"3.1319N, 101.6841E\",\n",
    "                                **get_rpi_system_metrics()\n",
    "                            },\n",
    "                            \"env\": env_data,\n",
    "                            \"viewer_id\": vid,\n",
    "                            \"summary\": {\n",
    "                                \"appearance_count\": stats[\"count\"],\n",
    "                                \"avg_age_est\": avg_age,\n",
    "                                \"avg_age_score\": avg_age_score,\n",
    "                                \"avg_gender_score\": avg_gender_score,\n",
    "                                \"majority_gender\": majority_gender,\n",
    "                                \"avg_emotion_score\": avg_emotion_score,\n",
    "                                \"majority_emotion\": majority_emotion,\n",
    "                                \"avg_attention_duration\": avg_attention,\n",
    "                                \"gaze_at_screen_ratio\": gaze_ratio\n",
    "                            }\n",
    "                        }\n",
    "                \n",
    "                        logger.info(json.dumps(summary_out))\n",
    "                        expired_viewers.append(vid)\n",
    "                \n",
    "                # -------- Cleanup expired viewers --------\n",
    "                for vid in expired_viewers:\n",
    "                    del viewer_summaries[vid]\n",
    "    \n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error occurred during the inference loop\")\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Execution interrupted by user.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Unhandled exception occurred in the main execution block\")\n",
    "finally:\n",
    "    try:\n",
    "        video_source.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "        logger.info(\"Resources released. Exiting.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error occurred during resource cleanup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_digital_signage_env)",
   "language": "python",
   "name": "ai_digital_signage_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
