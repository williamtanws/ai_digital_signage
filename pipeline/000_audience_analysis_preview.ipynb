{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:degirum.zoo_manager:Local inference with local zoo from '../models' dir\n",
      "INFO:degirum.zoo_manager:Local inference with local zoo from '../models' dir\n",
      "INFO:degirum.zoo_manager:Local inference with local zoo from '../models' dir\n",
      "INFO:degirum.zoo_manager:Local inference with local zoo from '../models' dir\n",
      "INFO:degirum.zoo_manager:Local inference with local zoo from '../models' dir\n",
      "[1:51:24.428782919] [11602] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:326 \u001b[0mlibcamera v0.5.0+59-d83ff0a4\n",
      "INFO:picamera2.picamera2:Initialization successful.\n",
      "[1:51:24.436922060] [11693] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:720 \u001b[0mlibpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)\n",
      "[1:51:24.447677021] [11693] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1179 \u001b[0mRegistered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a to CFE device /dev/media0 and ISP device /dev/media2 using PiSP variant BCM2712_D0\n",
      "INFO:picamera2.picamera2:Camera now open.\n",
      "INFO:picamera2.picamera2:Camera configuration has been adjusted!\n",
      "INFO:picamera2.picamera2:Configuration successful!\n",
      "[1:51:24.455863162] [11602] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1205 \u001b[0mconfiguring streams: (0) 640x480-RGB888 (1) 1536x864-BGGR_PISP_COMP1\n",
      "[1:51:24.457343551] [11693] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1483 \u001b[0mSensor: /base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a - Selected sensor format: 1536x864-SBGGR10_1X10 - Selected CFE format: 1536x864-PC1B\n",
      "INFO:picamera2.picamera2:Camera started\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Audience Analysis Script using Hailo-8, DeGirum SDK, and PiCamera2\n",
    "# ----------------------------------------------------------\n",
    "# Models: Face Detection, Age, Gender, Emotion, Embedding\n",
    "# Hardware: Raspberry Pi 5 + Hailo-8 + Camera Module 3\n",
    "# Filename: 000_audience_analysis_preview.ipynb\n",
    "# Created date: 01 July 2025\n",
    "# Last modified date: 06 July 2025\n",
    "# Version: 1.0.0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Sample JSON Output (Single Viewer Record)\n",
    "# ----------------------------------------------------------\n",
    "# {\n",
    "#   \"age_est\": 50,\n",
    "#   \"age_score\": 49.81,\n",
    "#   \"attention_duration\": 3.4,\n",
    "#   \"emotion\": \"neutral\",\n",
    "#   \"emotion_score\": 0.93,\n",
    "#   \"env\": {\n",
    "#     \"aqi_est\": 115,\n",
    "#     \"humidity\": 65.1,\n",
    "#     \"temp_c\": 29.5\n",
    "#   },\n",
    "#   \"gaze_at_screen\": true,\n",
    "#   \"gender\": \"Female\",\n",
    "#   \"gender_score\": 0.98,\n",
    "#   \"location\": {\n",
    "#     \"coordinates\": \"3.1319N, 101.6841E\",\n",
    "#     \"mac_address\": \"88:A2:9E:1C:49:6F\"\n",
    "#   },\n",
    "#   \"timestamp\": \"2025-07-06T09:23:06.647690Z\",\n",
    "#   \"viewer_id\": \"unknown\"\n",
    "# }\n",
    "\n",
    "import degirum as dg\n",
    "import degirum_tools\n",
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import random\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"audience_analysis_preview\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Camera Streaming Class (PiCamera2 with FPS control)\n",
    "# ----------------------------------------------------------\n",
    "class CameraStream:\n",
    "    def __init__(self, fps=5):  # Limit camera to 5 FPS for performance\n",
    "        self.picam2 = Picamera2()\n",
    "        self.fps_interval = 1.0 / fps\n",
    "        self.picam2.configure(self.picam2.create_preview_configuration(\n",
    "            main={\"format\": \"RGB888\"}\n",
    "        ))\n",
    "        self.picam2.start(show_preview=False)\n",
    "        time.sleep(2)  # Allow sensor to stabilize\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "            frame = self.picam2.capture_array()\n",
    "            yield frame\n",
    "            elapsed = time.time() - start_time\n",
    "            time.sleep(max(0.0, self.fps_interval - elapsed))  # Frame pacing\n",
    "\n",
    "    def stop(self):\n",
    "        try:\n",
    "            self.picam2.stop()\n",
    "            print(\"Camera stopped.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Camera Stop Error] {e}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Inference Configuration\n",
    "# ----------------------------------------------------------\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"../models\"\n",
    "token = \"\"\n",
    "device_type = \"HAILORT/HAILO8\"\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Model Selection\n",
    "# ----------------------------------------------------------\n",
    "face_det_model_name     = \"retinaface_mobilenet--736x1280_quant_hailort_hailo8_1\"\n",
    "face_embed_model_name   = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8_1\"\n",
    "age_model_name          = \"yolov8n_relu6_age--256x256_quant_hailort_hailo8_1\"\n",
    "gender_model_name       = \"yolov8n_relu6_fairface_gender--256x256_quant_hailort_hailo8_1\"\n",
    "emotion_cls_model       = \"emotion_recognition_fer2013--64x64_quant_hailort_multidevice_1\"\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Models\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Load Face Detection Model\n",
    "face_det_model = dg.load_model(\n",
    "    model_name=face_det_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "face_det_model.overlay_color = [(255, 255, 0), (0, 255, 0)]\n",
    "\n",
    "# Load Face Embedding Model\n",
    "face_embed_model = dg.load_model(\n",
    "    model_name=face_embed_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# Load Age Estimation Model\n",
    "age_model = dg.load_model(\n",
    "    model_name=age_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# Load Gender Classification Model\n",
    "gender_model = dg.load_model(\n",
    "    model_name=gender_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# Load Emotion Recognition Model\n",
    "emotion_cls_model = dg.load_model(\n",
    "    model_name=emotion_cls_model,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Create Compound Models (Face Crop + Classification)\n",
    "# ----------------------------------------------------------\n",
    "face_emotion_model    = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, emotion_cls_model)\n",
    "face_age_model        = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, age_model)\n",
    "face_gender_model     = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, gender_model)\n",
    "face_embed_model_comp = degirum_tools.CroppingAndClassifyingCompoundModel(face_det_model, face_embed_model)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Utility: Generate Unique Viewer ID from Face Embedding\n",
    "# ----------------------------------------------------------\n",
    "def get_viewer_id(embedding):\n",
    "    if not embedding:\n",
    "        return \"unknown\"\n",
    "    h = hashlib.md5(str(embedding).encode()).hexdigest()\n",
    "    return h[:8]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Utility: Draw Demographic Labels on Frame\n",
    "# ----------------------------------------------------------\n",
    "def draw_overlay(image, emotion_results, age_results, gender_results):\n",
    "    for i in range(len(emotion_results)):\n",
    "        try:\n",
    "            bbox = emotion_results[i].get(\"bbox\", [])\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "\n",
    "            age           = round(age_results[i].get(\"score\", 0))\n",
    "            age_score     = age_results[i].get(\"score\", 0.0)\n",
    "            gender        = gender_results[i].get(\"label\", \"\")\n",
    "            gender_score  = gender_results[i].get(\"score\", 0.0)\n",
    "            emotion       = emotion_results[i].get(\"label\", \"\")\n",
    "            emotion_score = emotion_results[i].get(\"score\", 0.0)\n",
    "\n",
    "            label = f\"{gender} ({gender_score:.2f}) | Age: {age} ({age_score:.2f}) | {emotion} ({emotion_score:.2f})\"\n",
    "\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)\n",
    "            cv2.rectangle(image, (x1, y1 - 22), (x1 + w, y1), (0, 255, 255), -1)\n",
    "            cv2.putText(image, label, (x1, y1 - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1)\n",
    "        except Exception as e:\n",
    "            print(f\"[Overlay Error] Face {i}: {e}\")\n",
    "    return image\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Utility: Get MAC Address as Device ID\n",
    "# ----------------------------------------------------------\n",
    "def get_mac_address():\n",
    "    mac = uuid.getnode()\n",
    "    mac_str = ':'.join(f'{(mac >> ele) & 0xff:02x}' for ele in range(40, -1, -8))\n",
    "    return mac_str.upper()\n",
    "\n",
    "mac_address = get_mac_address()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Generator: Run Multi-Model Inference per Frame\n",
    "# ----------------------------------------------------------\n",
    "def run_inference(video_source):\n",
    "    for frame in video_source:\n",
    "        emotion_result = face_emotion_model.predict(frame)\n",
    "        age_result     = face_age_model.predict(frame)\n",
    "        gender_result  = face_gender_model.predict(frame)\n",
    "        embed_result   = face_embed_model_comp.predict(frame)\n",
    "        yield frame, emotion_result, age_result, gender_result, embed_result\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Main Runtime\n",
    "# ----------------------------------------------------------\n",
    "video_source = CameraStream(fps=5)\n",
    "\n",
    "try:\n",
    "    with degirum_tools.Display(\"Audience Analysis\") as display:\n",
    "        for frame, emotion_result, age_result, gender_result, embed_result in run_inference(video_source):\n",
    "\n",
    "            if not emotion_result.results:\n",
    "                continue  # Skip frame if no face detected\n",
    "\n",
    "            base_image = emotion_result.image\n",
    "            overlayed_image = draw_overlay(\n",
    "                base_image,\n",
    "                emotion_result.results,\n",
    "                age_result.results,\n",
    "                gender_result.results\n",
    "            )\n",
    "            display.show(overlayed_image)\n",
    "\n",
    "            # Align results length across all models\n",
    "            min_len = min(\n",
    "                len(emotion_result.results),\n",
    "                len(age_result.results),\n",
    "                len(gender_result.results),\n",
    "                len(embed_result.results)\n",
    "            )\n",
    "\n",
    "            for i in range(min_len):\n",
    "                try:\n",
    "                    embedding = embed_result.results[i].get(\"embedding\", [])\n",
    "                    viewer_id = get_viewer_id(embedding)\n",
    "\n",
    "                    json_output = {\n",
    "                        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                        \"location\": {\n",
    "                            \"mac_address\": mac_address,\n",
    "                            \"coordinates\": \"3.1319N, 101.6841E\"\n",
    "                        },\n",
    "                        \"env\": {\n",
    "                            \"temp_c\": 29.5,\n",
    "                            \"humidity\": 65.1,\n",
    "                            \"aqi_est\": 115\n",
    "                        },\n",
    "                        \"viewer_id\": viewer_id,\n",
    "                        \"age_est\": round(age_result.results[i].get(\"score\", 0)),\n",
    "                        \"age_score\": age_result.results[i].get(\"score\", 0.0),\n",
    "                        \"gender\": gender_result.results[i].get(\"label\", \"\"),\n",
    "                        \"gender_score\": gender_result.results[i].get(\"score\", 0.0),\n",
    "                        \"emotion\": emotion_result.results[i].get(\"label\", \"\"),\n",
    "                        \"emotion_score\": emotion_result.results[i].get(\"score\", 0.0),\n",
    "                        \"attention_duration\": round(random.uniform(2.0, 7.5), 1),\n",
    "                        \n",
    "                        # NOTE: Placeholder simulation. Replace with real gaze/attention inference if available.\n",
    "                        \"gaze_at_screen\": random.choice([True, False])\n",
    "                    }\n",
    "\n",
    "                    # NOTE: Debug purpose\n",
    "                    #print(json.dumps(json_output, separators=(\",\", \":\")))\n",
    "                    #print(json.dumps(json_output, indent=2))  # for human-readable debug\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[JSON Output Error] Face {i}: {e}\")\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    video_source.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (degirum_env)",
   "language": "python",
   "name": "degirum_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
