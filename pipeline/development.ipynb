{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6edef2-9ff3-4e9c-a6a2-6a347ac5a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-07 18:56:38,931 [INFO] === Log initialized: /home/william/ai_digital_signage/logs/audience_analysis_live.log ===\n",
      "2026-02-07 18:56:38,936 [INFO] Log file size: 188888 bytes\n",
      "2026-02-07 18:56:39,015 [INFO] BME688 sensor initialized (secondary)\n",
      "2026-02-07 18:56:39,024 [INFO] Loading models...\n",
      "2026-02-07 18:56:39,162 [INFO] Loaded WiderFace model\n",
      "2026-02-07 18:56:39,226 [INFO] Loaded embedding model\n",
      "2026-02-07 18:56:39,286 [INFO] Loaded age model\n",
      "2026-02-07 18:56:39,338 [INFO] Loaded gender model\n",
      "2026-02-07 18:56:39,395 [INFO] Loaded emotion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1:03:42.329078827] [46599] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:330 \u001b[0mlibcamera v0.5.2+99-bfd68f78\n",
      "[1:03:42.336616009] [46685] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:720 \u001b[0mlibpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)\n",
      "[1:03:42.339318909] [46685] \u001b[1;32m INFO \u001b[1;37mIPAProxy \u001b[1;34mipa_proxy.cpp:180 \u001b[0mUsing tuning file /usr/share/libcamera/ipa/rpi/pisp/imx708.json\n",
      "[1:03:42.346801757] [46685] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:220 \u001b[0mAdding camera '/base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a' for pipeline handler rpi/pisp\n",
      "[1:03:42.346818943] [46685] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1179 \u001b[0mRegistered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a to CFE device /dev/media0 and ISP device /dev/media2 using PiSP variant BCM2712_D0\n",
      "[1:03:42.351684873] [46599] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1215 \u001b[0mconfiguring streams: (0) 640x480-RGB888/sRGB (1) 1536x864-BGGR_PISP_COMP1/RAW\n",
      "[1:03:42.351820928] [46685] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1483 \u001b[0mSensor: /base/axi/pcie@1000120000/rp1/i2c@88000/imx708@1a - Selected sensor format: 1536x864-SBGGR10_1X10/RAW - Selected CFE format: 1536x864-PC1B/RAW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-07 18:56:40,452 [INFO] Threaded camera started at 640x480 @ 10fps\n",
      "2026-02-07 18:56:40,457 [INFO] Demographics worker started (queue_size=8)\n",
      "2026-02-07 18:56:40,574 [INFO] Display available\n",
      "2026-02-07 18:56:40,582 [INFO] ============================================================\n",
      "2026-02-07 18:56:40,589 [INFO] GAZE TRACKING v3.0.1 STARTED\n",
      "2026-02-07 18:56:40,596 [INFO]   Camera: 640x480 @ 10fps\n",
      "2026-02-07 18:56:40,602 [INFO]   Skip frames: 3 | Max faces: 3\n",
      "2026-02-07 18:56:40,610 [INFO]   Min loop time: 100ms\n",
      "2026-02-07 18:56:40,617 [INFO]   Thermal: throttle=78.0°C critical=82.0°C\n",
      "2026-02-07 18:56:40,622 [INFO]   Gaze: Yaw ±20° Pitch ±15° Min 0.5s\n",
      "2026-02-07 18:56:40,629 [INFO]   Log file: /home/william/ai_digital_signage/logs/audience_analysis_live.log\n",
      "2026-02-07 18:56:40,636 [INFO] ============================================================\n",
      "2026-02-07 18:56:45,952 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:56:45.952749Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.6, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.43, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 62.4}}\n",
      "2026-02-07 18:56:45,995 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=61°C\n",
      "2026-02-07 18:56:50,985 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:56:50.985528Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.0, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.43, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 54.4}}\n",
      "2026-02-07 18:56:50,990 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:56:56,118 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:56:56.118542Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.0, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.44, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 57.2}}\n",
      "2026-02-07 18:56:56,124 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:57:00,843 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:00.843770Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.6, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.44, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 57.2}}\n",
      "2026-02-07 18:57:00,886 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=61°C\n",
      "2026-02-07 18:57:06,188 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:06.188111Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.0, \"environment\": {\"temp_c\": 31.97, \"humidity\": 55.55, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 55.6}}\n",
      "2026-02-07 18:57:06,231 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:57:10,851 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:10.851485Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 59.5, \"environment\": {\"temp_c\": 31.97, \"humidity\": 55.55, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 55.6}}\n",
      "2026-02-07 18:57:10,894 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:57:16,127 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:16.127031Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 0, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 59.5, \"environment\": {\"temp_c\": 31.97, \"humidity\": 55.55, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 55.2}}\n",
      "2026-02-07 18:57:16,131 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=0 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:57:20,013 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:19.967485Z\", \"event\": \"session_end\", \"viewer_id\": \"ec97ef2e\", \"session_stats\": {\"viewer_id\": \"ec97ef2e\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 3.72, \"total_appearances\": 1, \"engagement_rate\": 0.0}, \"demographics\": {\"age\": 0, \"gender\": \"\", \"emotions\": {}}, \"environment\": {\"temp_c\": 31.98, \"humidity\": 55.59, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0}}\n",
      "2026-02-07 18:57:20,017 [INFO] SESSION END: ec97ef2e gaze=0.0s dur=3.7s engage=0.0%\n",
      "2026-02-07 18:57:21,147 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:21.147468Z\", \"event\": \"session_end\", \"viewer_id\": \"0aa4813a\", \"session_stats\": {\"viewer_id\": \"0aa4813a\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 3.17, \"total_appearances\": 1, \"engagement_rate\": 0.0}, \"demographics\": {\"age\": 0, \"gender\": \"\", \"emotions\": {}}, \"environment\": {\"temp_c\": 31.98, \"humidity\": 55.59, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0}}\n",
      "2026-02-07 18:57:21,152 [INFO] SESSION END: 0aa4813a gaze=0.0s dur=3.2s engage=0.0%\n",
      "2026-02-07 18:57:21,455 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:21.455101Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 1, \"total_faces_detected\": 3, \"total_gaze_events\": 0, \"fps\": 8.9, \"cpu_temp\": 60.6, \"environment\": {\"temp_c\": 31.98, \"humidity\": 55.59, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 56.5}}\n",
      "2026-02-07 18:57:21,459 [INFO] HEARTBEAT: gazers=0 tracked=1 faces_total=3 gaze_events=0 FPS=8.9 CPU=61°C\n",
      "2026-02-07 18:57:26,433 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:26.433898Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 1, \"total_faces_detected\": 19, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 59.5, \"environment\": {\"temp_c\": 31.98, \"humidity\": 55.59, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 55.7}}\n",
      "2026-02-07 18:57:26,439 [INFO] HEARTBEAT: gazers=0 tracked=1 faces_total=19 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:57:31,556 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:31.556294Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 1, \"total_faces_detected\": 27, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 61.1, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.4, \"pressure_hPa\": 1002.56, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 73.1}}\n",
      "2026-02-07 18:57:31,601 [INFO] HEARTBEAT: gazers=0 tracked=1 faces_total=27 gaze_events=0 FPS=10.0 CPU=61°C\n",
      "2026-02-07 18:57:31,711 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:31.711040Z\", \"event\": \"session_end\", \"viewer_id\": \"6ce73625\", \"session_stats\": {\"viewer_id\": \"6ce73625\", \"total_gaze_time\": 0.0, \"gaze_count\": 0, \"session_duration\": 11.48, \"total_appearances\": 25, \"engagement_rate\": 0.0}, \"demographics\": {\"age\": 21, \"gender\": \"Male\", \"emotions\": {\"neutral\": 1}}, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.4, \"pressure_hPa\": 1002.56, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 73.1}}\n",
      "2026-02-07 18:57:31,715 [INFO] SESSION END: 6ce73625 gaze=0.0s dur=11.5s engage=0.0%\n",
      "2026-02-07 18:57:36,587 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:36.587682Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 27, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 59.0, \"environment\": {\"temp_c\": 32.0, \"humidity\": 55.4, \"pressure_hPa\": 1002.56, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 77.2}}\n",
      "2026-02-07 18:57:36,632 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=27 gaze_events=0 FPS=10.0 CPU=59°C\n",
      "2026-02-07 18:57:41,650 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:41.649918Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 27, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.6, \"environment\": {\"temp_c\": 32.02, \"humidity\": 55.35, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 74.4}}\n",
      "2026-02-07 18:57:41,693 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=27 gaze_events=0 FPS=10.0 CPU=61°C\n",
      "2026-02-07 18:57:46,665 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:46.665059Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 27, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.6, \"environment\": {\"temp_c\": 32.02, \"humidity\": 55.35, \"pressure_hPa\": 1002.54, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 78.7}}\n",
      "2026-02-07 18:57:46,708 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=27 gaze_events=0 FPS=10.0 CPU=61°C\n",
      "2026-02-07 18:57:51,727 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:51.727678Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 27, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 59.5, \"environment\": {\"temp_c\": 32.05, \"humidity\": 55.2, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 68.8}}\n",
      "2026-02-07 18:57:51,734 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=27 gaze_events=0 FPS=10.0 CPU=60°C\n",
      "2026-02-07 18:57:56,714 [INFO] GAZE_EVENT: {\"timestamp\": \"2026-02-07T10:57:56.714864Z\", \"event\": \"heartbeat\", \"active_gazers\": 0, \"tracked_viewers\": 0, \"total_faces_detected\": 27, \"total_gaze_events\": 0, \"fps\": 10.0, \"cpu_temp\": 60.6, \"environment\": {\"temp_c\": 32.05, \"humidity\": 55.2, \"pressure_hPa\": 1002.55, \"gas_resistance_ohms\": 102400000.0, \"noise_db\": 75.8}}\n",
      "2026-02-07 18:57:56,763 [INFO] HEARTBEAT: gazers=0 tracked=0 faces_total=27 gaze_events=0 FPS=10.0 CPU=61°C\n",
      "2026-02-07 18:58:00,752 [INFO] Interrupted\n",
      "2026-02-07 18:58:00,798 [INFO] Shutting down...\n",
      "2026-02-07 18:58:00,863 [INFO] Clean shutdown. Total faces=27 gaze_events=0\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Audience Attention and Gaze Analysis Module\n",
    "# ----------------------------------------------------------\n",
    "# Description:\n",
    "# This module implements real-time audience attention analysis for an\n",
    "# edge AI–powered digital signage system. Viewer attention is operationalised\n",
    "# through a composite Attention Indicator derived from gaze direction,\n",
    "# gaze duration, and face presence stability, enabling objective measurement\n",
    "# of on-screen engagement.\n",
    "#\n",
    "# In addition to visual analytics, ambient noise level is captured using\n",
    "# an external USB microphone as a contextual environmental indicator.\n",
    "# Noise measurements provide supplementary situational awareness of the\n",
    "# deployment environment (e.g., background activity and crowd conditions)\n",
    "# and are not used for speech recognition or audio content analysis.\n",
    "#\n",
    "# Functional Scope:\n",
    "# - Face detection and multi-face tracking using a WiderFace-based model\n",
    "#   with five-point facial landmark localisation\n",
    "# - Gaze estimation based on head pose and facial landmarks to infer\n",
    "#   on-screen viewing behaviour\n",
    "# - Attention Indicator computation combining gaze persistence and\n",
    "#   temporal face stability to mitigate transient or incidental detections\n",
    "# - Ambient noise level measurement for environmental context analysis\n",
    "# - Demographic estimation including age and gender classification\n",
    "# - Facial emotion recognition for affective context analysis\n",
    "# - Face embedding generation for short-term, privacy-preserving\n",
    "#   viewer session identification\n",
    "#\n",
    "# System Platform:\n",
    "# - Edge computing device: Raspberry Pi 5 (16 GB RAM)\n",
    "# - AI accelerator: Hailo-8 (26 TOPS)\n",
    "# - Image acquisition: Raspberry Pi Camera Module 3\n",
    "# - Audio input: USB microphone (ambient noise level measurement only)\n",
    "#\n",
    "# Research Context:\n",
    "# This module forms part of an AI-powered digital signage system evaluated\n",
    "# through controlled in-house experimentation and field-aligned testing\n",
    "# within Malaysian SME food and beverage (F&B) environments. The collected\n",
    "# visual and environmental indicators support quantitative analysis of\n",
    "# audience attention and engagement while adhering to privacy-by-design\n",
    "# principles.\n",
    "#\n",
    "# File: 000_audience_gaze_analysis.py\n",
    "# Created: 07 February 2026\n",
    "# Version: 1.0.0\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "import threading\n",
    "import numpy as np\n",
    "import degirum as dg\n",
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "from datetime import datetime\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import bme680\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from hailo_platform import Device\n",
    "import sys\n",
    "from collections import deque\n",
    "import sounddevice as sd\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------------------------\n",
    "preview_camera = True\n",
    "console_output = True\n",
    "\n",
    "# Performance — tuned for RPi 5 + Hailo-8\n",
    "SKIP_FRAMES = 3\n",
    "CAMERA_FPS = 10\n",
    "PREVIEW_WIDTH = 640\n",
    "PREVIEW_HEIGHT = 480\n",
    "MAX_FACES_PER_FRAME = 3\n",
    "MIN_LOOP_TIME = 0.100\n",
    "DEMO_QUEUE_SIZE = 8\n",
    "\n",
    "# Thermal management\n",
    "THERMAL_THROTTLE_TEMP = 78.0\n",
    "THERMAL_CRITICAL_TEMP = 82.0\n",
    "THERMAL_CHECK_INTERVAL = 3.0\n",
    "\n",
    "# Gaze detection\n",
    "GAZE_YAW_THRESHOLD = 20\n",
    "GAZE_PITCH_THRESHOLD = 15\n",
    "MIN_GAZE_DURATION = 0.5\n",
    "ENGAGEMENT_TIMEOUT = 3.0\n",
    "\n",
    "# Noise level (USB microphone)\n",
    "NOISE_SAMPLE_RATE = 16000   # Hz\n",
    "NOISE_DURATION = 0.2        # seconds\n",
    "NOISE_REF_PRESSURE = 20e-6  # Reference sound pressure (20 µPa)\n",
    "NOISE_READ_INTERVAL = 5.0   # seconds\n",
    "\n",
    "# Logging\n",
    "LOG_INTERVAL = 5.0\n",
    "OUTPUT_DIR = \"../output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# DeGirum / Hailo\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"../models\"\n",
    "token = \"\"\n",
    "device_type = \"HAILORT/HAILO8\"\n",
    "\n",
    "widerface_model_name = \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8_1\"\n",
    "face_embed_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8_1\"\n",
    "age_model_name = \"yolov8n_relu6_age--256x256_quant_hailort_hailo8_1\"\n",
    "gender_model_name = \"yolov8n_relu6_fairface_gender--256x256_quant_hailort_hailo8_1\"\n",
    "emotion_model_name = \"emotion_recognition_fer2013--64x64_quant_hailort_multidevice_1\"\n",
    "\n",
    "EMB_DIM = 128\n",
    "\n",
    "viewer_profiles = {}\n",
    "_ZERO_EMB = np.zeros(EMB_DIM, dtype=np.float32)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Logging — Single file: audience_analysis_live.log\n",
    "#\n",
    "# Uses a flushing handler so every log line hits disk immediately.\n",
    "# This prevents \"empty log\" issues on RPi where the process may\n",
    "# be killed before Python's internal buffers flush.\n",
    "# ----------------------------------------------------------\n",
    "LOG_DIR = \"../logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "LOG_FILE = f\"{LOG_DIR}/audience_analysis_live.log\"\n",
    "\n",
    "\n",
    "class FlushingTimedRotatingFileHandler(TimedRotatingFileHandler):\n",
    "    \"\"\"TimedRotatingFileHandler that flushes + fsync after every emit.\"\"\"\n",
    "    def emit(self, record):\n",
    "        super().emit(record)\n",
    "        try:\n",
    "            self.flush()\n",
    "            if hasattr(self.stream, 'fileno'):\n",
    "                os.fsync(self.stream.fileno())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"audience_analysis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "logger.propagate = False  # Don't send to root logger\n",
    "\n",
    "_fmt = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "_fh = FlushingTimedRotatingFileHandler(\n",
    "    LOG_FILE, when=\"H\", interval=1, backupCount=168, utc=True\n",
    ")\n",
    "_fh.setFormatter(_fmt)\n",
    "logger.addHandler(_fh)\n",
    "\n",
    "if console_output:\n",
    "    _ch = logging.StreamHandler(sys.stdout)  # Explicit stdout\n",
    "    _ch.setFormatter(_fmt)\n",
    "    logger.addHandler(_ch)\n",
    "\n",
    "# Verify log file is writable\n",
    "logger.info(f\"=== Log initialized: {os.path.abspath(LOG_FILE)} ===\")\n",
    "if os.path.exists(LOG_FILE):\n",
    "    logger.info(f\"Log file size: {os.path.getsize(LOG_FILE)} bytes\")\n",
    "else:\n",
    "    logger.warning(f\"Log file NOT created at {LOG_FILE} — check permissions\")\n",
    "\n",
    "\n",
    "def log_gaze_event(data):\n",
    "    \"\"\"Log structured JSON event. Guaranteed flush to disk.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"GAZE_EVENT: {json.dumps(data)}\")\n",
    "    except (TypeError, ValueError) as e:\n",
    "        logger.warning(f\"Serialize error: {e}\")\n",
    "\n",
    "\n",
    "def handle_uncaught_exceptions(exc_type, exc_value, exc_tb):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_tb)\n",
    "        return\n",
    "    logger.critical(\"Uncaught Exception\", exc_info=(exc_type, exc_value, exc_tb))\n",
    "\n",
    "sys.excepthook = handle_uncaught_exceptions\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Thermal Monitor\n",
    "# ----------------------------------------------------------\n",
    "_thermal_cache = {\"temp\": 45.0, \"ts\": 0}\n",
    "\n",
    "\n",
    "def get_cpu_temp():\n",
    "    now = time.time()\n",
    "    if now - _thermal_cache[\"ts\"] < THERMAL_CHECK_INTERVAL:\n",
    "        return _thermal_cache[\"temp\"]\n",
    "    try:\n",
    "        with open(\"/sys/class/thermal/thermal_zone0/temp\") as f:\n",
    "            _thermal_cache[\"temp\"] = int(f.read().strip()) / 1000.0\n",
    "    except Exception:\n",
    "        pass\n",
    "    _thermal_cache[\"ts\"] = now\n",
    "    return _thermal_cache[\"temp\"]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# BME688 Sensor\n",
    "# ----------------------------------------------------------\n",
    "def set_bme688_sensor(sensor):\n",
    "    sensor.set_humidity_oversample(bme680.OS_2X)\n",
    "    sensor.set_pressure_oversample(bme680.OS_4X)\n",
    "    sensor.set_temperature_oversample(bme680.OS_8X)\n",
    "    sensor.set_filter(bme680.FILTER_SIZE_3)\n",
    "    sensor.set_gas_status(bme680.ENABLE_GAS_MEAS)\n",
    "\n",
    "bme_sensor = None\n",
    "bme_data_cache = {\"temp_c\": None, \"humidity\": None, \"pressure_hPa\": None, \"gas_resistance_ohms\": None}\n",
    "bme_last_read = 0\n",
    "BME_READ_INTERVAL = 10.0\n",
    "\n",
    "try:\n",
    "    bme_sensor = bme680.BME680(bme680.I2C_ADDR_PRIMARY)\n",
    "    set_bme688_sensor(bme_sensor)\n",
    "    logger.info(\"BME688 sensor initialized\")\n",
    "except (RuntimeError, IOError):\n",
    "    try:\n",
    "        bme_sensor = bme680.BME680(bme680.I2C_ADDR_SECONDARY)\n",
    "        set_bme688_sensor(bme_sensor)\n",
    "        logger.info(\"BME688 sensor initialized (secondary)\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"BME688 not available: {e}\")\n",
    "\n",
    "\n",
    "def read_bme688_data():\n",
    "    global bme_data_cache, bme_last_read\n",
    "    if bme_sensor is None:\n",
    "        return bme_data_cache\n",
    "    now = time.time()\n",
    "    if now - bme_last_read < BME_READ_INTERVAL:\n",
    "        return bme_data_cache\n",
    "    try:\n",
    "        if bme_sensor.get_sensor_data():\n",
    "            bme_data_cache = {\n",
    "                \"temp_c\": round(bme_sensor.data.temperature, 2),\n",
    "                \"humidity\": round(bme_sensor.data.humidity, 2),\n",
    "                \"pressure_hPa\": round(bme_sensor.data.pressure, 2),\n",
    "                \"gas_resistance_ohms\": round(bme_sensor.data.gas_resistance, 2)\n",
    "            }\n",
    "            bme_last_read = now\n",
    "    except Exception:\n",
    "        pass\n",
    "    return bme_data_cache\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Ambient Noise Level (USB Microphone)\n",
    "# ----------------------------------------------------------\n",
    "_noise_cache = {\"db\": None, \"ts\": 0.0}\n",
    "\n",
    "def read_noise_level_db():\n",
    "    \"\"\"\n",
    "    Measure ambient noise level in decibels (dB) using RMS amplitude.\n",
    "    This function captures a short audio buffer and computes an\n",
    "    approximate sound pressure level for environmental context only.\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    if now - _noise_cache[\"ts\"] < NOISE_READ_INTERVAL:\n",
    "        return _noise_cache[\"db\"]\n",
    "\n",
    "    try:\n",
    "        audio = sd.rec(\n",
    "            int(NOISE_SAMPLE_RATE * NOISE_DURATION),\n",
    "            samplerate=NOISE_SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32',\n",
    "            blocking=True\n",
    "        )\n",
    "\n",
    "        rms = np.sqrt(np.mean(np.square(audio)))\n",
    "        if rms > 0:\n",
    "            db = 20 * np.log10(rms / NOISE_REF_PRESSURE)\n",
    "            db = round(float(db), 1)\n",
    "        else:\n",
    "            db = 0.0\n",
    "\n",
    "        _noise_cache.update({\"db\": db, \"ts\": now})\n",
    "        return db\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Noise read failed: {e}\")\n",
    "        return _noise_cache[\"db\"]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Threaded Camera Capture\n",
    "# ----------------------------------------------------------\n",
    "class ThreadedCamera:\n",
    "    def __init__(self, fps=10, width=640, height=480):\n",
    "        self.picam2 = Picamera2()\n",
    "        config = self.picam2.create_preview_configuration(\n",
    "            main={\"format\": \"RGB888\", \"size\": (width, height)},\n",
    "            controls={\"FrameRate\": fps}\n",
    "        )\n",
    "        self.picam2.configure(config)\n",
    "        self.picam2.start(show_preview=False)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        self._frame = None\n",
    "        self._lock = threading.Lock()\n",
    "        self._running = True\n",
    "        self._interval = 1.0 / fps\n",
    "\n",
    "        self._thread = threading.Thread(target=self._capture_loop, daemon=True, name=\"cam-capture\")\n",
    "        self._thread.start()\n",
    "        logger.info(f\"Threaded camera started at {width}x{height} @ {fps}fps\")\n",
    "\n",
    "    def _capture_loop(self):\n",
    "        while self._running:\n",
    "            try:\n",
    "                start = time.time()\n",
    "                frame = self.picam2.capture_array()\n",
    "                with self._lock:\n",
    "                    self._frame = frame\n",
    "                elapsed = time.time() - start\n",
    "                sleep = max(0, self._interval - elapsed)\n",
    "                if sleep > 0:\n",
    "                    time.sleep(sleep)\n",
    "            except Exception:\n",
    "                if self._running:\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "    def read(self):\n",
    "        with self._lock:\n",
    "            return self._frame\n",
    "\n",
    "    def stop(self):\n",
    "        self._running = False\n",
    "        self._thread.join(timeout=2.0)\n",
    "        try:\n",
    "            self.picam2.stop()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Utilities\n",
    "# ----------------------------------------------------------\n",
    "def cosine_distance(a, b):\n",
    "    d = np.dot(a, b)\n",
    "    n = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return 1.0 - (d / n) if n > 0 else 1.0\n",
    "\n",
    "\n",
    "def parse_keypoints(kpts_data):\n",
    "    if kpts_data is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(kpts_data, np.ndarray):\n",
    "            if kpts_data.shape == (5, 2):\n",
    "                return kpts_data.astype(np.float32)\n",
    "            if kpts_data.ndim == 1 and kpts_data.size == 10:\n",
    "                return kpts_data.reshape(5, 2).astype(np.float32)\n",
    "        elif isinstance(kpts_data, (list, tuple)):\n",
    "            a = np.array(kpts_data, dtype=np.float32)\n",
    "            if a.size == 10:\n",
    "                return a.reshape(5, 2)\n",
    "            if a.shape == (5, 2):\n",
    "                return a\n",
    "        elif isinstance(kpts_data, dict):\n",
    "            if 'data' in kpts_data:\n",
    "                return parse_keypoints(kpts_data['data'])\n",
    "            if 'x' in kpts_data and 'y' in kpts_data:\n",
    "                x = np.array(kpts_data['x'], dtype=np.float32)\n",
    "                y = np.array(kpts_data['y'], dtype=np.float32)\n",
    "                if len(x) == 5 and len(y) == 5:\n",
    "                    return np.column_stack([x, y])\n",
    "        elif isinstance(kpts_data, list) and kpts_data and isinstance(kpts_data[0], dict):\n",
    "            c = []\n",
    "            for pt in kpts_data[:5]:\n",
    "                if 'x' in pt and 'y' in pt:\n",
    "                    c.append([float(pt['x']), float(pt['y'])])\n",
    "            if len(c) == 5:\n",
    "                return np.array(c, dtype=np.float32)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_embedding(emb_data):\n",
    "    try:\n",
    "        if isinstance(emb_data, np.ndarray):\n",
    "            return emb_data.flatten().astype(np.float32)\n",
    "        if isinstance(emb_data, dict):\n",
    "            for k in (\"data\", \"embedding\", \"vector\"):\n",
    "                v = emb_data.get(k)\n",
    "                if v is not None:\n",
    "                    if isinstance(v, np.ndarray):\n",
    "                        return v.flatten().astype(np.float32)\n",
    "                    if isinstance(v, list) and v:\n",
    "                        if isinstance(v[0], list):\n",
    "                            v = v[0]\n",
    "                        return np.array(v, dtype=np.float32)\n",
    "        if isinstance(emb_data, list) and emb_data:\n",
    "            if isinstance(emb_data[0], list):\n",
    "                emb_data = emb_data[0]\n",
    "            return np.array(emb_data, dtype=np.float32)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return _ZERO_EMB\n",
    "\n",
    "\n",
    "def draw_overlay(image, faces_data):\n",
    "    for face in faces_data:\n",
    "        try:\n",
    "            bbox = face.get(\"bbox\")\n",
    "            if not bbox:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            gazing = face.get(\"is_gazing\", False)\n",
    "            dur = face.get(\"gaze_duration\", 0.0)\n",
    "\n",
    "            color = (0, 255, 0) if gazing else (0, 255, 255)\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 3 if gazing else 2)\n",
    "\n",
    "            age = face.get(\"age_est\", 0)\n",
    "            gen = face.get(\"gender\", \"\")\n",
    "            emo = face.get(\"emotion\", \"\")\n",
    "            label = f\"GAZING {dur:.1f}s | {gen} {age}y | {emo}\" if gazing else f\"{gen} {age}y | {emo}\"\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            (tw, th), bl = cv2.getTextSize(label, font, 0.5, 1)\n",
    "            cv2.rectangle(image, (x1, y1 - th - bl - 5), (x1 + tw + 5, y1), color, -1)\n",
    "            cv2.putText(image, label, (x1 + 2, y1 - bl - 2), font, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "            if gazing:\n",
    "                yaw, pitch = face.get(\"yaw\"), face.get(\"pitch\")\n",
    "                if yaw is not None and pitch is not None:\n",
    "                    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                    ax = int(cx - 40 * np.sin(np.radians(yaw)))\n",
    "                    ay = int(cy + 40 * np.sin(np.radians(pitch)))\n",
    "                    cv2.arrowedLine(image, (cx, cy), (ax, ay), (0, 255, 0), 2, tipLength=0.3)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_mac_address():\n",
    "    mac = uuid.getnode()\n",
    "    return \":\".join(f\"{(mac >> i) & 0xff:02x}\" for i in range(40, -1, -8)).upper()\n",
    "\n",
    "mac_address = get_mac_address()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Head Pose (pre-computed for fixed resolution)\n",
    "# ----------------------------------------------------------\n",
    "_PTS3D = np.array([\n",
    "    [-30, 35, 30], [30, 35, 30], [0, 0, 60],\n",
    "    [-25, -35, 20], [25, -35, 20]\n",
    "], dtype=np.float32)\n",
    "_DIST = np.zeros(5, dtype=np.float32)\n",
    "_f = 0.9 * PREVIEW_WIDTH\n",
    "_K = np.array([[_f, 0, PREVIEW_WIDTH / 2],\n",
    "               [0, _f, PREVIEW_HEIGHT / 2],\n",
    "               [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "def head_pose_from_5pts(pts2d):\n",
    "    if pts2d is None:\n",
    "        return None, None, None\n",
    "    try:\n",
    "        if not isinstance(pts2d, np.ndarray) or pts2d.shape != (5, 2):\n",
    "            return None, None, None\n",
    "        ok, rvec, tvec = cv2.solvePnP(_PTS3D, pts2d, _K, _DIST, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "        if not ok:\n",
    "            return None, None, None\n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "        sy = np.sqrt(R[0, 0]**2 + R[1, 0]**2)\n",
    "        return (np.degrees(np.arctan2(R[1, 0], R[0, 0])),\n",
    "                np.degrees(np.arctan2(-R[2, 0], sy)),\n",
    "                np.degrees(np.arctan2(R[2, 1], R[2, 2])))\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def is_gazing_at_screen(pts2d):\n",
    "    yaw, pitch, _ = head_pose_from_5pts(pts2d)\n",
    "    if yaw is None:\n",
    "        return False, None, None\n",
    "    return (abs(yaw) <= GAZE_YAW_THRESHOLD and abs(pitch) <= GAZE_PITCH_THRESHOLD), yaw, pitch\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Viewer Tracker\n",
    "# ----------------------------------------------------------\n",
    "class ViewerTracker:\n",
    "    def __init__(self, iou_thr=0.3, emb_thr=0.5, timeout=5.0):\n",
    "        self.iou_thr = iou_thr\n",
    "        self.emb_thr = emb_thr\n",
    "        self.timeout = timeout\n",
    "        self.tracks = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _iou(a, b):\n",
    "        xA, yA = max(a[0], b[0]), max(a[1], b[1])\n",
    "        xB, yB = min(a[2], b[2]), min(a[3], b[3])\n",
    "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "        union = (a[2]-a[0])*(a[3]-a[1]) + (b[2]-b[0])*(b[3]-b[1]) - inter\n",
    "        return inter / union if union > 0 else 0\n",
    "\n",
    "    def _clean(self):\n",
    "        now = time.time()\n",
    "        stale = [k for k, v in self.tracks.items() if now - v['ts'] > self.timeout]\n",
    "        for k in stale:\n",
    "            del self.tracks[k]\n",
    "\n",
    "    def update(self, bboxes, embs):\n",
    "        self._clean()\n",
    "        T = list(self.tracks.keys())\n",
    "        N, M = len(T), len(bboxes)\n",
    "        now = time.time()\n",
    "\n",
    "        if N == 0:\n",
    "            out = []\n",
    "            for bb, emb in zip(bboxes, embs):\n",
    "                nid = uuid.uuid4().hex[:8]\n",
    "                self.tracks[nid] = {'bbox': bb, 'emb': emb, 'ts': now}\n",
    "                out.append((nid, True))\n",
    "            return out\n",
    "\n",
    "        cost = np.zeros((N, M), dtype=np.float32)\n",
    "        for i, tid in enumerate(T):\n",
    "            t = self.tracks[tid]\n",
    "            for j in range(M):\n",
    "                cost[i, j] = 0.4 * (1 - self._iou(t['bbox'], bboxes[j])) + 0.6 * cosine_distance(t['emb'], embs[j])\n",
    "\n",
    "        rows, cols = linear_sum_assignment(cost)\n",
    "        results = [None] * M\n",
    "\n",
    "        for r, c in zip(rows, cols):\n",
    "            tid = T[r]\n",
    "            t = self.tracks[tid]\n",
    "            if self._iou(t['bbox'], bboxes[c]) >= self.iou_thr or cosine_distance(t['emb'], embs[c]) <= self.emb_thr:\n",
    "                t.update({'bbox': bboxes[c], 'emb': embs[c], 'ts': now})\n",
    "                results[c] = (tid, False)\n",
    "\n",
    "        for j in range(M):\n",
    "            if results[j] is None:\n",
    "                nid = uuid.uuid4().hex[:8]\n",
    "                self.tracks[nid] = {'bbox': bboxes[j], 'emb': embs[j], 'ts': now}\n",
    "                results[j] = (nid, True)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Gaze Session Manager (FIXED)\n",
    "# ----------------------------------------------------------\n",
    "class GazeSessionManager:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}\n",
    "\n",
    "    def update(self, vid, is_gazing, ts):\n",
    "        if vid not in self.sessions:\n",
    "            self.sessions[vid] = {\n",
    "                'total': 0.0, 'start': None, 'prev': ts,\n",
    "                'sess_start': ts, 'last_seen': ts, 'count': 0, 'appearances': 0\n",
    "            }\n",
    "        s = self.sessions[vid]\n",
    "        s['appearances'] += 1\n",
    "        s['prev'] = ts\n",
    "        s['last_seen'] = ts\n",
    "\n",
    "        continuous = 0.0\n",
    "        if is_gazing:\n",
    "            if s['start'] is None:\n",
    "                s['start'] = ts\n",
    "                s['count'] += 1\n",
    "            continuous = ts - s['start']\n",
    "        else:\n",
    "            if s['start'] is not None:\n",
    "                dur = ts - s['start']\n",
    "                if dur >= MIN_GAZE_DURATION:\n",
    "                    s['total'] += dur\n",
    "                s['start'] = None\n",
    "        return s['total'], continuous, True\n",
    "\n",
    "    def end_session(self, vid):\n",
    "        if vid not in self.sessions:\n",
    "            return None\n",
    "        s = self.sessions[vid]\n",
    "        now = time.time()\n",
    "        if s['start'] is not None:\n",
    "            dur = now - s['start']\n",
    "            if dur >= MIN_GAZE_DURATION:\n",
    "                s['total'] += dur\n",
    "        elapsed = now - s['sess_start']\n",
    "        stats = {\n",
    "            'viewer_id': vid,\n",
    "            'total_gaze_time': round(s['total'], 2),\n",
    "            'gaze_count': s['count'],\n",
    "            'session_duration': round(elapsed, 2),\n",
    "            'total_appearances': s['appearances'],\n",
    "            'engagement_rate': round(s['total'] / elapsed, 3) if elapsed > 0 else 0\n",
    "        }\n",
    "        del self.sessions[vid]\n",
    "        return stats\n",
    "\n",
    "    def cleanup_stale(self):\n",
    "        now = time.time()\n",
    "        stale = [v for v, s in self.sessions.items() if now - s['last_seen'] > ENGAGEMENT_TIMEOUT]\n",
    "        ended = []\n",
    "        for vid in stale:\n",
    "            st = self.end_session(vid)\n",
    "            if st:  # Log ALL ended sessions, not just those with gaze > 0\n",
    "                ended.append(st)\n",
    "        return ended\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Background Demographics Thread\n",
    "# ----------------------------------------------------------\n",
    "class DemographicsWorker:\n",
    "    def __init__(self, age_mdl, gender_mdl, emotion_mdl, queue_size=8):\n",
    "        self._age_mdl = age_mdl\n",
    "        self._gender_mdl = gender_mdl\n",
    "        self._emotion_mdl = emotion_mdl\n",
    "        self._queue = deque(maxlen=queue_size)\n",
    "        self._lock = threading.Lock()\n",
    "        self._running = True\n",
    "        self._thread = threading.Thread(target=self._run, daemon=True, name=\"demo-worker\")\n",
    "        self._thread.start()\n",
    "        logger.info(f\"Demographics worker started (queue_size={queue_size})\")\n",
    "\n",
    "    def submit(self, viewer_id, crop):\n",
    "        with self._lock:\n",
    "            self._queue.append((viewer_id, crop.copy()))\n",
    "\n",
    "    def _run(self):\n",
    "        while self._running:\n",
    "            job = None\n",
    "            with self._lock:\n",
    "                if self._queue:\n",
    "                    job = self._queue.popleft()\n",
    "            if job is None:\n",
    "                time.sleep(0.05)\n",
    "                continue\n",
    "            vid, crop = job\n",
    "            try:\n",
    "                self._process(vid, crop)\n",
    "            except Exception:\n",
    "                logger.debug(f\"Demographics failed for {vid}\", exc_info=True)\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    def _process(self, vid, crop):\n",
    "        if crop is None or crop.size == 0:\n",
    "            return\n",
    "\n",
    "        age_val, gen_val, emo_val = 0, \"\", \"\"\n",
    "\n",
    "        try:\n",
    "            r = self._age_mdl.predict(crop)\n",
    "            if hasattr(r, 'results') and r.results:\n",
    "                d = r.results[0]\n",
    "                raw = d.get(\"score\", 0) if isinstance(d, dict) else getattr(d, \"score\", 0)\n",
    "                age_val = round(raw) if raw else 0\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            r = self._gender_mdl.predict(crop)\n",
    "            if hasattr(r, 'results') and r.results:\n",
    "                d = r.results[0]\n",
    "                gen_val = d.get(\"label\", \"\") if isinstance(d, dict) else getattr(d, \"label\", \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            r = self._emotion_mdl.predict(crop)\n",
    "            if hasattr(r, 'results') and r.results:\n",
    "                d = r.results[0]\n",
    "                emo_val = d.get(\"label\", \"\") if isinstance(d, dict) else getattr(d, \"label\", \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if vid in viewer_profiles:\n",
    "            if age_val > 0:\n",
    "                viewer_profiles[vid]['age'] = age_val\n",
    "            if gen_val:\n",
    "                viewer_profiles[vid]['gender'] = gen_val\n",
    "            if emo_val:\n",
    "                viewer_profiles[vid]['emotions'][emo_val] = viewer_profiles[vid]['emotions'].get(emo_val, 0) + 1\n",
    "            logger.debug(f\"Demo result: {vid} age={age_val} gender={gen_val} emotion={emo_val}\")\n",
    "\n",
    "    def stop(self):\n",
    "        self._running = False\n",
    "        self._thread.join(timeout=3.0)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Models\n",
    "# ----------------------------------------------------------\n",
    "logger.info(\"Loading models...\")\n",
    "try:\n",
    "    widerface_model = dg.load_model(\n",
    "        model_name=widerface_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded WiderFace model\")\n",
    "\n",
    "    face_embed_model = dg.load_model(\n",
    "        model_name=face_embed_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded embedding model\")\n",
    "\n",
    "    age_model = dg.load_model(\n",
    "        model_name=age_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded age model\")\n",
    "\n",
    "    gender_model = dg.load_model(\n",
    "        model_name=gender_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded gender model\")\n",
    "\n",
    "    emotion_model = dg.load_model(\n",
    "        model_name=emotion_model_name, inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url, token=token, device_type=device_type)\n",
    "    logger.info(\"Loaded emotion model\")\n",
    "\n",
    "except Exception:\n",
    "    logger.exception(\"Failed to load models\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Initialize Components\n",
    "# ----------------------------------------------------------\n",
    "camera = ThreadedCamera(fps=CAMERA_FPS, width=PREVIEW_WIDTH, height=PREVIEW_HEIGHT)\n",
    "tracker = ViewerTracker(iou_thr=0.25, emb_thr=0.5, timeout=3.0)\n",
    "gaze_mgr = GazeSessionManager()\n",
    "demo_worker = DemographicsWorker(age_model, gender_model, emotion_model, queue_size=DEMO_QUEUE_SIZE)\n",
    "\n",
    "display_available = preview_camera\n",
    "if preview_camera:\n",
    "    try:\n",
    "        cv2.namedWindow(\"test\", cv2.WINDOW_NORMAL)\n",
    "        cv2.destroyWindow(\"test\")\n",
    "        logger.info(\"Display available\")\n",
    "    except cv2.error:\n",
    "        logger.warning(\"No display — preview disabled\")\n",
    "        display_available = False\n",
    "\n",
    "fps_q = deque(maxlen=30)\n",
    "last_fps_t = time.time()\n",
    "last_log_t = time.time()\n",
    "frame_counter = 0\n",
    "total_faces_detected = 0\n",
    "total_gaze_events = 0\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"GAZE TRACKING v3.0.1 STARTED\")\n",
    "logger.info(f\"  Camera: {PREVIEW_WIDTH}x{PREVIEW_HEIGHT} @ {CAMERA_FPS}fps\")\n",
    "logger.info(f\"  Skip frames: {SKIP_FRAMES} | Max faces: {MAX_FACES_PER_FRAME}\")\n",
    "logger.info(f\"  Min loop time: {MIN_LOOP_TIME*1000:.0f}ms\")\n",
    "logger.info(f\"  Thermal: throttle={THERMAL_THROTTLE_TEMP}°C critical={THERMAL_CRITICAL_TEMP}°C\")\n",
    "logger.info(f\"  Gaze: Yaw ±{GAZE_YAW_THRESHOLD}° Pitch ±{GAZE_PITCH_THRESHOLD}° Min {MIN_GAZE_DURATION}s\")\n",
    "logger.info(f\"  Log file: {os.path.abspath(LOG_FILE)}\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Main Loop\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    while True:\n",
    "        loop_start = time.time()\n",
    "\n",
    "        frame = camera.read()\n",
    "        if frame is None:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        frame_counter += 1\n",
    "        current_time = time.time()\n",
    "        faces = []\n",
    "        was_processed = False\n",
    "\n",
    "        dt = current_time - last_fps_t\n",
    "        fps_q.append(1.0 / dt if dt > 0 else 0)\n",
    "        last_fps_t = current_time\n",
    "        avg_fps = sum(fps_q) / len(fps_q) if fps_q else 0\n",
    "\n",
    "        # ---- INFERENCE (on processed frames only) ----\n",
    "        if frame_counter % SKIP_FRAMES == 0:\n",
    "            cpu_temp = get_cpu_temp()\n",
    "\n",
    "            if cpu_temp >= THERMAL_CRITICAL_TEMP:\n",
    "                logger.warning(f\"CRITICAL TEMP {cpu_temp:.0f}°C — sleeping 1s\")\n",
    "                time.sleep(1.0)\n",
    "            elif cpu_temp >= THERMAL_THROTTLE_TEMP:\n",
    "                logger.info(f\"THROTTLE {cpu_temp:.0f}°C — sleeping 200ms\")\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            if cpu_temp < THERMAL_CRITICAL_TEMP:\n",
    "                try:\n",
    "                    det = widerface_model.predict(frame)\n",
    "                    was_processed = True\n",
    "\n",
    "                    if hasattr(det, 'results'):\n",
    "                        det_items = []\n",
    "                        for d in det.results:\n",
    "                            try:\n",
    "                                bbox = d.get(\"bbox\") if isinstance(d, dict) else getattr(d, \"bbox\", None)\n",
    "                                if bbox:\n",
    "                                    x1, y1, x2, y2 = bbox\n",
    "                                    det_items.append(((x2-x1)*(y2-y1), d))\n",
    "                            except Exception:\n",
    "                                continue\n",
    "                        det_items.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                        for _, d in det_items[:MAX_FACES_PER_FRAME]:\n",
    "                            try:\n",
    "                                bbox = d.get(\"bbox\") if isinstance(d, dict) else getattr(d, \"bbox\", None)\n",
    "                                x1, y1, x2, y2 = map(int, bbox)\n",
    "                                x1, y1 = max(0, x1), max(0, y1)\n",
    "                                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                                if x2 <= x1 or y2 <= y1:\n",
    "                                    continue\n",
    "\n",
    "                                crop = frame[y1:y2, x1:x2]\n",
    "                                if isinstance(d, dict):\n",
    "                                    kr = d.get(\"kpts\") or d.get(\"landmarks\") or d.get(\"keypoints\")\n",
    "                                else:\n",
    "                                    kr = getattr(d, \"kpts\", None) or getattr(d, \"landmarks\", None) or getattr(d, \"keypoints\", None)\n",
    "                                kpts = parse_keypoints(kr)\n",
    "\n",
    "                                emb_raw = {}\n",
    "                                if crop.size > 0:\n",
    "                                    try:\n",
    "                                        er = face_embed_model.predict(crop)\n",
    "                                        if hasattr(er, 'results') and er.results:\n",
    "                                            emb_raw = er.results[0]\n",
    "                                    except Exception:\n",
    "                                        pass\n",
    "\n",
    "                                faces.append({\n",
    "                                    \"bbox\": bbox, \"kpts\": kpts, \"embedding\": emb_raw,\n",
    "                                    \"crop\": crop,\n",
    "                                    \"age_est\": 0, \"gender\": \"\", \"gender_score\": 0.0,\n",
    "                                    \"emotion\": \"\", \"emotion_score\": 0.0,\n",
    "                                })\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "                    total_faces_detected += len(faces)\n",
    "\n",
    "                except Exception:\n",
    "                    logger.exception(\"Detection failed\")\n",
    "\n",
    "        # ---- TRACKING + GAZE ----\n",
    "        if was_processed and faces:\n",
    "            bboxes = [f[\"bbox\"] for f in faces]\n",
    "            embs = [extract_embedding(f.get(\"embedding\", {})) for f in faces]\n",
    "            assignments = tracker.update(bboxes, embs)\n",
    "\n",
    "            if len(assignments) != len(faces):\n",
    "                logger.warning(f\"Track mismatch: {len(assignments)} vs {len(faces)}\")\n",
    "            else:\n",
    "                for i, f in enumerate(faces):\n",
    "                    try:\n",
    "                        vid, is_new = assignments[i]\n",
    "\n",
    "                        if is_new:\n",
    "                            crop = f.get(\"crop\")\n",
    "                            if crop is not None and crop.size > 0:\n",
    "                                demo_worker.submit(vid, crop)\n",
    "\n",
    "                        f.pop(\"crop\", None)\n",
    "\n",
    "                        kpts = f.get(\"kpts\")\n",
    "                        if kpts is not None and isinstance(kpts, np.ndarray) and kpts.shape == (5, 2):\n",
    "                            gazing, yaw, pitch = is_gazing_at_screen(kpts)\n",
    "                        else:\n",
    "                            gazing, yaw, pitch = False, None, None\n",
    "\n",
    "                        total_g, cont_g, _ = gaze_mgr.update(vid, gazing, current_time)\n",
    "\n",
    "                        f[\"is_gazing\"] = gazing\n",
    "                        f[\"gaze_duration\"] = cont_g\n",
    "                        f[\"total_gaze\"] = total_g\n",
    "                        f[\"yaw\"] = yaw\n",
    "                        f[\"pitch\"] = pitch\n",
    "                        f[\"viewer_id\"] = vid\n",
    "\n",
    "                        if vid not in viewer_profiles or is_new:\n",
    "                            viewer_profiles[vid] = {\n",
    "                                'age': 0, 'gender': '', 'first_seen': current_time, 'emotions': {}\n",
    "                            }\n",
    "\n",
    "                        prof = viewer_profiles[vid]\n",
    "                        if prof['age'] > 0:\n",
    "                            f[\"age_est\"] = prof['age']\n",
    "                        if prof['gender']:\n",
    "                            f[\"gender\"] = prof['gender']\n",
    "\n",
    "                        emo = f.get(\"emotion\", \"\")\n",
    "                        if not emo and prof.get('emotions'):\n",
    "                            emo = max(prof['emotions'].items(), key=lambda x: x[1])[0]\n",
    "                            f[\"emotion\"] = emo\n",
    "\n",
    "                        if gazing and cont_g < 0.5:\n",
    "                            total_gaze_events += 1\n",
    "                            log_gaze_event({\n",
    "                                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                                'event': 'gaze_start',\n",
    "                                'viewer_id': vid,\n",
    "                                'demographics': {'age': prof['age'], 'gender': prof['gender'], 'emotion': emo},\n",
    "                                'head_pose': {\n",
    "                                    'yaw': round(yaw, 2) if yaw is not None else None,\n",
    "                                    'pitch': round(pitch, 2) if pitch is not None else None\n",
    "                                }\n",
    "                            })\n",
    "                            logger.info(f\"GAZE START: {vid} ({prof['gender']} {prof['age']}y)\")\n",
    "\n",
    "                    except Exception:\n",
    "                        logger.debug(f\"Face {i} error\", exc_info=True)\n",
    "                        continue\n",
    "\n",
    "        # ---- PERIODIC HEARTBEAT + ACTIVE GAZERS (always runs) ----\n",
    "        if current_time - last_log_t >= LOG_INTERVAL:\n",
    "            active = []\n",
    "            for vid, s in gaze_mgr.sessions.items():\n",
    "                if s['start'] is not None:\n",
    "                    ct = current_time - s['start']\n",
    "                    if ct >= MIN_GAZE_DURATION:\n",
    "                        p = viewer_profiles.get(vid, {})\n",
    "                        emos = p.get('emotions', {})\n",
    "                        active.append({\n",
    "                            'viewer_id': vid,\n",
    "                            'continuous_gaze': round(ct, 1),\n",
    "                            'total_gaze': round(s['total'], 1),\n",
    "                            'demographics': {\n",
    "                                'age': p.get('age', 0),\n",
    "                                'gender': p.get('gender', ''),\n",
    "                                'emotion': max(emos.items(), key=lambda x: x[1])[0] if emos else ''\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "            # ALWAYS log heartbeat — even with 0 gazers\n",
    "            env = read_bme688_data()\n",
    "            env[\"noise_db\"] = read_noise_level_db()\n",
    "            \n",
    "            heartbeat = {\n",
    "                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                'event': 'heartbeat',\n",
    "                'active_gazers': len(active),\n",
    "                'tracked_viewers': len(gaze_mgr.sessions),\n",
    "                'total_faces_detected': total_faces_detected,\n",
    "                'total_gaze_events': total_gaze_events,\n",
    "                'fps': round(avg_fps, 1),\n",
    "                'cpu_temp': round(get_cpu_temp(), 1),\n",
    "                'environment': env\n",
    "            }\n",
    "            if active:\n",
    "                heartbeat['gazers'] = active\n",
    "            log_gaze_event(heartbeat)\n",
    "            logger.info(f\"HEARTBEAT: gazers={len(active)} tracked={len(gaze_mgr.sessions)} faces_total={total_faces_detected} gaze_events={total_gaze_events} FPS={avg_fps:.1f} CPU={get_cpu_temp():.0f}°C\")\n",
    "\n",
    "            last_log_t = current_time\n",
    "\n",
    "        # ---- CLEANUP STALE SESSIONS ----\n",
    "        for ss in gaze_mgr.cleanup_stale():\n",
    "            vid = ss['viewer_id']\n",
    "            p = viewer_profiles.get(vid, {})\n",
    "            log_gaze_event({\n",
    "                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                'event': 'session_end', 'viewer_id': vid,\n",
    "                'session_stats': ss,\n",
    "                'demographics': {'age': p.get('age', 0), 'gender': p.get('gender', ''), 'emotions': p.get('emotions', {})},\n",
    "                'environment': read_bme688_data()\n",
    "            })\n",
    "            logger.info(f\"SESSION END: {vid} gaze={ss['total_gaze_time']:.1f}s dur={ss['session_duration']:.1f}s engage={ss['engagement_rate']:.1%}\")\n",
    "            viewer_profiles.pop(vid, None)\n",
    "\n",
    "        # ---- PREVIEW ----\n",
    "        if display_available:\n",
    "            disp = frame.copy()\n",
    "            if faces:\n",
    "                disp = draw_overlay(disp, faces)\n",
    "            n_gaze = sum(1 for f in faces if f.get(\"is_gazing\"))\n",
    "            t = get_cpu_temp()\n",
    "            cv2.putText(disp, f\"FPS:{avg_fps:.1f} F:{len(faces)} G:{n_gaze} T:{t:.0f}C\",\n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Audience Gaze Analysis\", disp)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                logger.info(\"User exit\")\n",
    "                break\n",
    "\n",
    "        # ---- ENFORCE MIN LOOP TIME ----\n",
    "        elapsed = time.time() - loop_start\n",
    "        if elapsed < MIN_LOOP_TIME:\n",
    "            time.sleep(MIN_LOOP_TIME - elapsed)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Interrupted\")\n",
    "finally:\n",
    "    logger.info(\"Shutting down...\")\n",
    "    for vid in list(gaze_mgr.sessions.keys()):\n",
    "        st = gaze_mgr.end_session(vid)\n",
    "        if st:\n",
    "            p = viewer_profiles.get(vid, {})\n",
    "            log_gaze_event({\n",
    "                'timestamp': datetime.utcnow().isoformat() + \"Z\",\n",
    "                'event': 'shutdown_session_end', 'viewer_id': vid,\n",
    "                'session_stats': st,\n",
    "                'demographics': {'age': p.get('age', 0), 'gender': p.get('gender', ''), 'emotions': p.get('emotions', {})}\n",
    "            })\n",
    "            logger.info(f\"Final: {vid} gaze={st['total_gaze_time']:.1f}s\")\n",
    "\n",
    "    demo_worker.stop()\n",
    "    camera.stop()\n",
    "    if display_available:\n",
    "        cv2.destroyAllWindows()\n",
    "    logger.info(f\"Clean shutdown. Total faces={total_faces_detected} gaze_events={total_gaze_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6943155-d595-4c9d-ac63-e5d2a98dad54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63427df7-e0c5-4fc4-a1ca-b644ae895620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_digital_signage_env)",
   "language": "python",
   "name": "ai_digital_signage_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
